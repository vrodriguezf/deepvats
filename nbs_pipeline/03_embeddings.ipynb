{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9de2cd87-7d93-45ec-bd92-d97a85cf73b3",
   "metadata": {},
   "source": [
    "# Getting the embeddings\n",
    "\n",
    "> This notebook gets the embeddings (or latent space) from a multivariate time series \n",
    "given by a encoder (e.g., autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db2cd5f-dd8d-42a3-bdc1-afc12ff80afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tchub.all import *\n",
    "from tsai.data.preparation import SlidingWindow\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "wandb_api = wandb.Api()\n",
    "from yaml import load, FullLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f280e30d-8195-4f39-898a-495d9253778b",
   "metadata": {},
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c114f4-ba8e-48ae-9a6d-b5dda96b717b",
   "metadata": {},
   "source": [
    "Put here everything that could be needed if this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "312f776f-e284-4ab8-b888-8caa723d7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    use_wandb = False, # Whether to use or not wandb for experiment tracking\n",
    "    wandb_group = 'embeddings', # Whether to group this run in a wandb group\n",
    "    wandb_entity = 'pacmel',\n",
    "    wandb_project = 'tchub',\n",
    "    enc_artifact = 'pacmel/tchub/mvp:run-tchub-2juemwx8', # Name:version of the encoder artifact\n",
    "    input_ar = None,\n",
    "    cpu = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fe624f-f4ff-4310-9d3c-23099381ed91",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa46f602-bbf9-4d2d-ae68-771adae56199",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(entity=config.wandb_entity,\n",
    "                    project=config.wandb_project if config.use_wandb else 'work-nbs', \n",
    "                    group=config.wandb_group,\n",
    "                    job_type='embeddings', \n",
    "                    mode='online' if config.use_wandb else 'disabled',\n",
    "                    anonymous = 'never' if config.use_wandb else 'must',\n",
    "                    config=config,\n",
    "                    #id = 'embeddingsProvider',\n",
    "                    resume='allow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26dac90a-ed13-4f50-b95e-fc78c635e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Botch to use artifacts offline\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08922829-19f7-4cdc-97b1-86f5a3b4f3d2",
   "metadata": {},
   "source": [
    "Restore the encoder model and its associated configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e978658-78de-4dff-9679-d2ed1453dec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_artifact = artifacts_gettr(config.enc_artifact, type='learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7fbe313-d3b6-4793-aaf3-34d4b858065c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f57ff058220>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: This only works when you run it two timeS! WTF?\n",
    "try:\n",
    "    enc_learner = enc_artifact.to_obj()\n",
    "except:\n",
    "    enc_learner = enc_artifact.to_obj()\n",
    "enc_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1dfa6-52b9-4114-a4f8-16a94b83b21c",
   "metadata": {},
   "source": [
    "Restore the dataset artifact used for training the encoder. Even if we do not compute the dimensionality reduction over this dataset, we need to know the metadata of the encoder training set, to check that \n",
    "it matches with the dataset that we want to reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4335e626-5faa-4d27-845c-015f23ab9375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('taxi:v0', 'taxi:v0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_run = enc_artifact.logged_by()\n",
    "enc_artifact_train = artifacts_gettr(enc_run.config['train_artifact'], type='dataset')\n",
    "enc_artifact_valid = artifacts_gettr(enc_run.config['valid_artifact'], type='dataset')\n",
    "enc_artifact_train.name, enc_artifact_valid.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918829a-dd1f-4ddb-915c-cebf41665160",
   "metadata": {},
   "source": [
    "Now we specify the dataset artifact that we want to get the embeddings from. If no \n",
    "artifact is defined, the artifact to reduce will be the one used for validate the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59ff4ef9-cabd-41cb-84c8-ec86cadf5848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taxi:v0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ar_name = ifnone(config.input_ar, \n",
    "                       f'{enc_artifact_valid.entity}/{enc_artifact_valid.project}/{enc_artifact_valid.name}')\n",
    "wandb.config.update({'input_ar': input_ar_name}, allow_val_change=True)\n",
    "input_ar = artifacts_gettr(input_ar_name)\n",
    "input_ar.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a81845-e56f-4558-b5a6-74a7a7ea569b",
   "metadata": {},
   "source": [
    "Now we need to check whether the artifact that is going to be used fort the dimensionality reduction matches the artifact used to train the encoder. Matching means having the same number of variables, the same window size and stride, and the same frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca55d239-27b6-4d88-bd06-e7c2acf0009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def check_compatibility(input_ar:TSArtifact, enc_ar:TSArtifact):\n",
    "    \"Function to check that the artifact used to train the encoder model and the artifact that is \\\n",
    "    going to be passed to do inference\"\n",
    "    try:\n",
    "        # Check that both artifacts have the same variables\n",
    "        chk_vars = input_ar.metadata['TS']['vars'] == enc_ar.metadata['TS']['vars']\n",
    "        # Check that both artifacts have the same freq\n",
    "        chk_freq = input_ar.metadata['TS']['freq'] == enc_ar.metadata['TS']['freq']\n",
    "        # Check that the dr artifact is not normalized (not normalized data has not the key normalization)\n",
    "        chk_norm = input_ar.metadata['TS'].get('normalization') is None\n",
    "        # Check that the dr artifact has not missing values\n",
    "        chk_miss = input_ar.metadata['TS']['has_missing_values'] == \"False\"\n",
    "        # Check all logical vars.\n",
    "        if chk_vars and chk_freq and chk_norm and chk_miss:\n",
    "            print(\"Artifacts are compatible.\")\n",
    "        else:\n",
    "            raise Exception\n",
    "    except Exception as e:\n",
    "        print(\"Artifacts are not compatible.\")\n",
    "        raise e\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84cf086-f6ee-4f04-a234-581a738fc338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-10-01 00:00:00</th>\n",
       "      <td>12751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-01 00:30:00</th>\n",
       "      <td>8767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-01 01:00:00</th>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-01 01:30:00</th>\n",
       "      <td>5257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-10-01 02:00:00</th>\n",
       "      <td>4189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-14 21:30:00</th>\n",
       "      <td>16344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-14 22:00:00</th>\n",
       "      <td>15913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-14 22:30:00</th>\n",
       "      <td>14327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-14 23:00:00</th>\n",
       "      <td>12060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-14 23:30:00</th>\n",
       "      <td>10952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     value\n",
       "timestamp                 \n",
       "2014-10-01 00:00:00  12751\n",
       "2014-10-01 00:30:00   8767\n",
       "2014-10-01 01:00:00   7005\n",
       "2014-10-01 01:30:00   5257\n",
       "2014-10-01 02:00:00   4189\n",
       "...                    ...\n",
       "2014-12-14 21:30:00  16344\n",
       "2014-12-14 22:00:00  15913\n",
       "2014-12-14 22:30:00  14327\n",
       "2014-12-14 23:00:00  12060\n",
       "2014-12-14 23:30:00  10952\n",
       "\n",
       "[3600 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = input_ar.to_df()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46c6bb02-9a43-4917-93a9-0832ee98b5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3600, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49908f03-6875-4e27-8050-09af41dc62bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3553, 1, 48)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input, _ = SlidingWindow(window_len=enc_run.config['w'], \n",
    "                             stride=enc_run.config['stride'], \n",
    "                             get_y=[])(df)\n",
    "enc_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08bfd363-3c5d-4f4d-a7e7-cb65c8bd9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/env/lib/python3.8/site-packages/torch/nn/modules/module.py:974: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.8 s, sys: 1.42 s, total: 6.22 s\n",
      "Wall time: 5.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embs = get_enc_embs(enc_input, enc_learner, cpu=config.cpu, to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3393cc12-f6c2-4bdc-adba-662689a9c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_wandb: \n",
    "    run.log_artifact(ReferenceArtifact(embs, 'embeddings', metadata=dict(run.config)), \n",
    "                     aliases=f'run-{run.project}-{run.id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2de38c4f-a266-45b5-a99f-0964f5a55113",
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
