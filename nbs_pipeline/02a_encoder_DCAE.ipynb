{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DCAE: Deep Convolutional Autoencoder\n",
    "\n",
    "> This notebook encodes the data using the autoencoder described in [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "Initial notebook setup and specific debugging and pre-configured cases selection.\n",
    "### VsCode update patch\n",
    "Initial notebook setup when using VSCode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if the notebook is run in VSCode\n",
    "import sys\n",
    "if '--vscode' in sys.argv:\n",
    "    print(\"Executing inside vscode\")\n",
    "    import nbs_pipeline.utils.vscode  as vs\n",
    "    vs.DisplayHandle.update = vs.update_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging variables\n",
    "\n",
    "- `print_flag`. If `True` it adds debbuging messages in those functions that allows so.\n",
    "- `reset_kernel`. If `True` it resets the kernel by the end of the execution. Use only in case that memory management is needed.\n",
    "- `check_memory_usage`. If `True`, it adds some lines for checking the GPU memmory ussage along the execution.\n",
    "- `time_flag`. If `True` it get the execution time along the notebook as well as inside those functions that allows so.\n",
    "- `window_size_percentage`. If `True`, MVP will be used directly with the proposed windows sizes. Otherwise, it will be asumed that they have been taken as absolute values and execution will be take that into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_flag = True\n",
    "reset_kernel = True\n",
    "check_memory_usage = True\n",
    "time_flag = True\n",
    "window_size_percentage = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preconfigurated cases selection\n",
    "- `pre_configured_case`. If `True`, a preconfigured case will be selected, forcing the artifact to get the expected configuration based on the information in `config\\*.yml` and `utils\\config.py`.\n",
    "- `case_id`. If `preconfigured_case` is `True`, it forces to select the configuration of the `case_id` preconfigured samples. The available preconfigured samples are shown in the next cell.\n",
    "- `frequency_factor`. If `pre_configured_case` is `True`, frequency will be resampled by `config.freq*frequency_factor`\n",
    "  `frequency_factor_change_alias`. If `pre_configured_case` is `True` and `frequency_factor != 1` then the dataset alias will be modified for adding the new frequency as suffix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      "0 - monash_australian_electricity_demand_0\n",
      "1 - monash_solar_4_seconds_0\n",
      "2 - wikipedia_0\n",
      "3 - traffic_san_francisco_0\n",
      "4 - monash_solar_10_minutes_0\n",
      "5 - etth1_0\n",
      "6 - stumpy_abp_0\n",
      "7 - stumpy_toy_0\n"
     ]
    }
   ],
   "source": [
    "import utils.config as cfg_\n",
    "cfg_.show_available_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_configured_case = True\n",
    "case_id = 1\n",
    "frequency_factor = 5\n",
    "frequency_factor_change_alias = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main code\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tsai.all import *\n",
    "except:\n",
    "    from tsai.all import * # TODO: Weird error when loading tsai!from tchub.all import *\n",
    "from fastcore.all import *\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import *\n",
    "from dvats.all import *\n",
    "import wandb\n",
    "if check_memory_usage: \n",
    "    import nbs_pipeline.utils.memory as mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "Before running this notebook, make sure you have the `$WANDB_API_KEY` environment varibale defined with your API_KEY (run in a terminal `echo $WANDB_API_KEY` to see it). If not, run in a terminal `wandb login [API_KEY]`. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Configurate Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api = wandb.Api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU | Used mem: 519\n",
      "GPU | Used mem: 24576\n",
      "GPU | Memory Usage: [\u001b[90m--------------------\u001b[0m] \u001b[90m2%\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cuda_device = 0\n",
    "device = torch.device(f'cuda:{cuda_device}' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)\n",
    "if check_memory_usage:\n",
    "    gpu_device = torch.cuda.current_device()\n",
    "    mem.gpu_memory_status(gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get configutation from yml\n",
    "> This file used the configuration files './config/base.yml' and './config/02a_encoder_DCAE.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Project configuration-----------\n",
      "user: mi-santamaria\n",
      "project: deepvats\n",
      "version: latest\n",
      "data: Monash-Australian_electricity_demand:latest\n",
      "-----------Project configuration-----------\n",
      "Current: /home/macu/work/nbs_pipeline\n",
      "yml: ./config/02a-encoder_dcae.yaml\n",
      "Getting content./config/02a-encoder_dcae.yaml\n",
      "... About to replace includes with content\n",
      "Load content./config/02a-encoder_dcae.yaml\n",
      "Antes de leer configuration {'include': None, 'user_preferences': {'use_wandb': True, 'wdb': {'user': 'mi-santamaria', 'project_name': 'deepvats', 'version': 'latest', 'mode': 'online', 'artifacts_path': './data/wandb_artifacts'}, 'data': {'folder': '~/data/', 'fname': 'australian_electricity_demand_dataset', 'ftype': '.tsf', 'cols': [0], 'freq': '1h'}, 'artifact': {'alias': 'Monash-Australian_electricity_demand', 'algorithm': 'mvp-SWV'}, 'directories': {'tmp': 'tmp', 'data': '~/data/australian_electricity_demand_dataset.tsf'}}, 'data': {'name': 'australian_electricity_demand_dataset', 'path': '~/data/australian_electricity_demand_dataset.tsf', 'alias': 'Monash-Australian_electricity_demand', 'cols': [0], 'csv_config': {}, 'date_offset': None, 'date_format': '%Y-%m-%d %H:%M:%S', 'freq': '1h', 'joining_train_test': False, 'missing_values': {'technique': None, 'constant': None}, 'normalize_training': False, 'range_training': None, 'range_testing': None, 'resampling_freq': None, 'start_date': None, 'test_split': None, 'time_col': None}, 'wandb': {'user': 'mi-santamaria', 'dir': '~/deepvats', 'enabled': False, 'group': None, 'log_learner': False, 'mode': 'online', 'project': 'deepvats', 'version': 'latest', 'artifacts_path': './data/wandb_artifacts'}, 'configuration': {'job_type': 'encoder_DCAE', 'alias': 'Monash-Australian_electricity_demand', 'wandb': {'use': True, 'entity': 'mi-santamaria', 'group': None, 'project': 'deepvats'}, 'artifacts': {'enc_prefix': 'mi-santamaria/deepvats/mvp-SWV', 'valid': {'data': None, 'size': 0.1}}, 'specifications': {'batch_size': 64, 'n_epoch': 200, 'pool_szs': [2, 2, 4], 'top_k': 3, 'sliding_windows': {'stride': 1, 'size': 32}, 'autoencoder': {'delta': 60, 'filters': {'nfs': [64, 32, 16], 'kss': [10, 5, 5], 'output_size': 10}}}}}\n",
      "\u001b[93m\u001b[1mresampling_freq is missing in original dict | 20S \u001b[0m\n",
      "\u001b[94mtrain_artifact: mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\u001b[0m -> mi-santamaria/deepvats/solar_4_seconds-20s:latest\u001b[0m\n",
      "\u001b[94mbatch_size: 64\u001b[0m -> 512\u001b[0m\n",
      "\u001b[93m\u001b[1mcsv_config is missing in original dict | {} \u001b[0m\n",
      "\u001b[93m\u001b[1mr is missing in original dict | 0.71 \u001b[0m\n",
      "\u001b[94mepochs: 200\u001b[0m -> 100\u001b[0m\n",
      "\u001b[94mstride: 1\u001b[0m -> 48\u001b[0m\n",
      "wandb_group: None\u001b[0m\n",
      "valid_artifact: None\u001b[0m\n",
      "pool_szs: [2, 2, 4]\u001b[0m\n",
      "delta: 60\u001b[0m\n",
      "\u001b[93m\u001b[1martifact_name is missing in original dict | solar_4_seconds-20s \u001b[0m\n",
      "\u001b[94malias: Monash-Australian_electricity_demand\u001b[0m -> solar_4_seconds\u001b[0m\n",
      "kss: [10, 5, 5]\u001b[0m\n",
      "top_k: [2, 2, 4]\u001b[0m\n",
      "\u001b[94moutput_filter_size: 10\u001b[0m -> [10, 5, 5]\u001b[0m\n",
      "\u001b[93m\u001b[1mtime_col is missing in original dict | None \u001b[0m\n",
      "wandb_project: deepvats\u001b[0m\n",
      "\u001b[93m\u001b[1mfreq is missing in original dict | 4s \u001b[0m\n",
      "use_wandb: True\u001b[0m\n",
      "wandb_entity: mi-santamaria\u001b[0m\n",
      "\u001b[94mw: 32\u001b[0m -> 236\u001b[0m\n",
      "\u001b[93m\u001b[1mdata_cols is missing in original dict | [] \u001b[0m\n",
      "nfs: [64, 32, 16]\u001b[0m\n",
      "\u001b[94mvalid_size: 0.1\u001b[0m -> 0.2\u001b[0m\n",
      "alias: solar_4_seconds\n",
      "use_wandb: True\n",
      "wandb_group: None\n",
      "wandb_entity: mi-santamaria\n",
      "wandb_project: deepvats\n",
      "train_artifact: mi-santamaria/deepvats/solar_4_seconds-20s:latest\n",
      "valid_artifact: None\n",
      "valid_size: 0.2\n",
      "w: 236\n",
      "stride: 48\n",
      "delta: 60\n",
      "nfs: [64, 32, 16]\n",
      "kss: [10, 5, 5]\n",
      "output_filter_size: [10, 5, 5]\n",
      "pool_szs: [2, 2, 4]\n",
      "batch_size: 512\n",
      "epochs: 100\n",
      "top_k: [2, 2, 4]\n",
      "artifact_name: solar_4_seconds-20s\n",
      "data_cols: []\n",
      "freq: 4s\n",
      "time_col: None\n",
      "csv_config: {}\n",
      "resampling_freq: 20S\n",
      "r: 0.71\n"
     ]
    }
   ],
   "source": [
    "import nbs_pipeline.utils.config as cfg\n",
    "dcae_config, job_type = cfg.get_artifact_config_DCAE(print_flag = print_flag)\n",
    "if pre_configured_case: \n",
    "    cfg_.force_artifact_config_dcae(\n",
    "        config = dcae_config,\n",
    "        id = case_id,\n",
    "        print_flag = print_flag, \n",
    "        both = print_flag,\n",
    "        frequency_factor = frequency_factor,\n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )\n",
    "\n",
    "if print_flag: cfg_.show_attrdict(dcae_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup W&B artiffact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runname: 02a_encoder_DCAE-sliding_window_view\n"
     ]
    }
   ],
   "source": [
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"02a_encoder_DCAE-sliding_window_view\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "print(\"runname: \"+runname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /home/macu/work/nbs_pipeline/02a_encoder_DCAE-sliding_window_view.ipynb.\n",
      "wandb: Currently logged in as: mi-santamaria. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macu/work/wandb/run-20240105_135254-0rn76t9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mi-santamaria/deepvats/runs/0rn76t9m' target=\"_blank\">02a_encoder_DCAE-sliding_window_view</a></strong> to <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mi-santamaria/deepvats/runs/0rn76t9m' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats/runs/0rn76t9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'stream.Stream' object attribute 'write' is read-only\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "run = wandb.init(\n",
    "    entity          = dcae_config.wandb_entity,\n",
    "    project         = dcae_config.wandb_project,\n",
    "    group           = dcae_config.wandb_group,\n",
    "    job_type        = job_type,\n",
    "    allow_val_change= True,\n",
    "    mode            = 'online' if dcae_config.use_wandb else 'disabled',\n",
    "    config          = dcae_config,\n",
    "    anonymous = 'never' if dcae_config.use_wandb else 'must', \n",
    "    resume=False,\n",
    "    name = runname\n",
    ")\n",
    "dcae_config = run.config  # Object for storing hyperparameters\n",
    "artifacts_gettr = run.use_artifact if dcae_config.use_wandb else wandb_api.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets\n",
    "\n",
    "To load the dataset we will download a specific dataset artifact from the collection of artifacts\n",
    "stored in the weights and biases (wandb) project associated to this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a continuous multivariate time-series data $D$ of dimension $d$ with $n$ time-steps, $D = X_1,X_2,\\dots,X_n$ , where each $X_i = \\{x_i^1,\\dots,x_i^d\\}$ . Let $w$ be the window width, $s$ the stride, and $t$ the start time of a sliding window in the data.\n",
    "\n",
    "Define a new matrix $Z_k$ where each row is a vector of size $w$ of data extracted from the $k^{th}$ dimension.\n",
    "\n",
    "\\begin{aligned}&Z_k(w,s,t)\\\\&\\quad =\\begin{bmatrix} x_{t}^k&\\quad x_{t+1}^k&\\quad \\dots&\\quad x_{t+w-1}^k \\\\ x_{t+s}^k&\\quad x_{t+s+1}^k&\\quad \\dots&\\quad x_{t+s+w-1}^k \\\\ \\vdots&\\quad \\vdots&\\quad \\ddots&\\quad \\vdots \\\\ x_{t+(r-1)s}^k&\\quad x_{t+(r-1)s+1}^k&\\quad \\dots&\\quad x_{t+(r-1)s+w-1}^k \\end{bmatrix} \\end{aligned}\n",
    "\n",
    "where $r$ is the number of desired rows, and $t+(r-1)s+w-1 \\le n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$ is a $w \\times s \\times t$ matrix. The first step consists in slicing the original multivariate time series into slices of shape ($w \\times d$), as shown in this figure from the paper.\n",
    "<img src=\"https://i.imgur.com/R9Fx8uO.png\" style=\"width:800px;height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of this sliding window approach are given values by default here. If the value has been already set previously, that means this notebook is being called from a wandb sweep, and we must use the value that the sweep is bringing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SlidingWindow(window_len=dcae_config.w, stride=dcae_config.stride, get_y=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1479445, 1), (30817, 1, 236))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_artifact = artifacts_gettr(dcae_config.train_artifact)\n",
    "df_train = train_artifact.to_df()\n",
    "X_train, _ = sw(df_train)\n",
    "df_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dcae_config.valid_artifact:\n",
    "    valid_artifact = artifacts_gettr(dcae_config.valid_artifact)\n",
    "    df_val = valid_artifact.to_df()\n",
    "    X_valid, _ = sw(df_val)\n",
    "    df_val.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important features from the multivariate time series data through Deep Convolutional Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional Auto Encoders (DCAE) is a powerful method for learning high-level and mid-level abstractions from low-level raw data. It has the ability to extract features from complex and large time-series in an unsupervised manner. This is useful to overcome the complexity of multivariate time-series.\n",
    "\n",
    "Compared to the conventional auto-encoder, DCAE has fewer parameters than the conventional auto-encoder which means less training time. Also, DCAE uses local information to reconstruct the signal while conventional auto-encoders utilize fully connected layers to globally do the reconstruction. DCAE is an unsupervised model for representation learning which maps inputs into a new representation space. It has two main parts which are the encoding part that is used to project the data into a set of feature spaces and the decoding part that reconstructs the original data. The latent space representation is the space where the data lie in the bottleneck layers.\n",
    "\n",
    "The loss function of the DCAE is defined as the error between the input and the output. DCAE aims to find a code for each input by minimizing the mean squared error (MSE) between its input (original data) and output (reconstructed data). The MSE is used which assists to minimize the loss; thus, the network is forced to learn a low-dimensional representation of the input.\n",
    "\n",
    "We will implement the DCAE of the paper [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y), whose architecture is shown in the table below:\n",
    "\n",
    "![](https://i.imgur.com/3EjuAfQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the paper, the input shape is $60 \\times 3$, due to multivariate time series has 3 variables and the window size is 60. Generally, the size of the input/output of the autoencoder will depend on the shape of each slice obtained in the previos step. The number of latent features to be discovered is $60$ in the table above, but we can consider this as a free hyperparameter $\\delta$. Also, according to the paper: \"*The number of feature maps, size of filter and depth of the model are set based on the reconstruction error on validation set.*\". Thus, we must provide flexibility in the creation of the DCAE in terms of these hyperparameters.ยบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are not using a config file, you can also uncomment the following cell and define the hyperparameters in the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_equal([len(x) for x in [dcae_config.nfs, dcae_config.kss, dcae_config.pool_szs]], \n",
    "          np.repeat(len(dcae_config.nfs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The implementation of the DCAE is done using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'AssertionError'>",
     "evalue": "The product of pool sizes must be a divisor of the window size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m \u001b[38;5;241m=\u001b[39m DCAE_torch(\n\u001b[1;32m      2\u001b[0m     c_in\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m      3\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39mdcae_config\u001b[38;5;241m.\u001b[39mw, \n\u001b[1;32m      4\u001b[0m     delta\u001b[38;5;241m=\u001b[39mdcae_config\u001b[38;5;241m.\u001b[39mdelta, \n\u001b[1;32m      5\u001b[0m     pool_szs\u001b[38;5;241m=\u001b[39mdcae_config\u001b[38;5;241m.\u001b[39mpool_szs, \n\u001b[1;32m      6\u001b[0m     nfs\u001b[38;5;241m=\u001b[39mdcae_config\u001b[38;5;241m.\u001b[39mnfs\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(m)\n\u001b[1;32m      9\u001b[0m foo \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dcae_config\u001b[38;5;241m.\u001b[39mw)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/meta.py:40\u001b[0m, in \u001b[0;36mPrePostInitMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(res)\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(res,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__pre_init__\u001b[39m\u001b[38;5;124m'\u001b[39m): res\u001b[38;5;241m.\u001b[39m__pre_init__(\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 40\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(res,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__post_init__\u001b[39m\u001b[38;5;124m'\u001b[39m): res\u001b[38;5;241m.\u001b[39m__post_init__(\u001b[38;5;241m*\u001b[39margs,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/work/dvats/encoder.py:70\u001b[0m, in \u001b[0;36mDCAE_torch.__init__\u001b[0;34m(self, c_in, seq_len, delta, nfs, kss, pool_szs, output_fsz)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m all_equal([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [nfs, kss, pool_szs]], np\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(nfs), \u001b[38;5;241m3\u001b[39m)), \\\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfs, kss, and pool_szs must have the same length\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mprod(pool_szs) \u001b[38;5;241m==\u001b[39m nfs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe number of filters in the last conv layer must be equal to the product of pool sizes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m seq_len \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(pool_szs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \\\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe product of pool sizes must be a divisor of the window size\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     72\u001b[0m layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m range_of(kss):\n",
      "\u001b[0;31mAssertionError\u001b[0m: The product of pool sizes must be a divisor of the window size"
     ]
    }
   ],
   "source": [
    "m = DCAE_torch(\n",
    "    c_in=X_train.shape[1], \n",
    "    seq_len=dcae_config.w, \n",
    "    delta=dcae_config.delta, \n",
    "    pool_szs=dcae_config.pool_szs, \n",
    "    nfs=dcae_config.nfs\n",
    ")\n",
    "print(m)\n",
    "foo = torch.rand(1, X_train.shape[1], dcae_config.w)\n",
    "foo.shape, m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with fastai Learner class, to abstract from Pytorch's training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dcae_config.valid_artifact:\n",
    "    X, y, splits  = combine_split_data(xs=[X_train, X_valid], ys=[X_train, X_valid])\n",
    "else:\n",
    "    X = X_train\n",
    "    y = X_train\n",
    "    splits = get_splits(np.arange(len(X)), valid_size=dcae_config.valid_size)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [ToFloat(), ToFloat()]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "dls.dataset[0][0].show(), ToTSTensor()(dls.dataset[0][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DCAE_torch(c_in=X_train.shape[1], seq_len=dcae_config.w, delta=dcae_config.delta, \n",
    "               pool_szs=dcae_config.pool_szs, nfs=dcae_config.nfs)\n",
    "learn = Learner(dls=dls, model=m, loss_func=nn.MSELoss(), opt_func=Adam, \n",
    "                cbs=[WandbCallback(log_preds=False)])\n",
    "lr_valley, lr_steep = learn.lr_find(suggest_funcs=[valley, steep])\n",
    "learn.fit_one_cycle(dcae_config.epochs, lr_max=lr_valley)\n",
    "learn.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "To track the performance of this model fit, go to the project dashboard in Weights & Biases. The link is provided at the beginning of this notebook, after the execution of the function `wandb.init()'' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, log the learner to be used by the next notebook in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_learn = learn.export_and_get()\n",
    "if config.use_wandb: \n",
    "    ar = ReferenceArtifact(aux_learn, f'dcae', type='learner', metadata=dict(run.dcae_config))\n",
    "    run.log_artifact(ar, aliases=f'run-{run.project}-{run.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models (To rewrite)\n",
    "\n",
    "Calculate baseline models taking into account that the best prediction is the average and median value of each of the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the autoencoder\n",
    "\n",
    "Let's validate the autoencoder quality visually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best and the worst k predictions using the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses = interp.top_losses(3)\n",
    "top_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in top_losses.indices: dls.dataset[i][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'NameError'>",
     "evalue": "name 'run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run' is not defined"
     ]
    }
   ],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
