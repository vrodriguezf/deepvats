{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| default_exp load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n",
    "\n",
    "> Methods for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "import wandb\n",
    "from datetime import datetime, timedelta\n",
    "from dvats.imports import *\n",
    "from dvats.utils import *\n",
    "import pickle\n",
    "import pyarrow.feather as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from tsai.imports import beep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/macu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = Path.home()\n",
    "base_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series artifacts (to be used with weights and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is meant to extend `wandb.Artifact` for logging/using files with time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "class TSArtifact(wandb.Artifact):\n",
    "\n",
    "    default_storage_path = Path(Path.home()/'data/wandb_artifacts/')\n",
    "    date_format = '%Y-%m-%d %H:%M:%S' # TODO add milliseconds\n",
    "    handle_missing_values_techniques = {\n",
    "        'linear_interpolation': lambda df : df.interpolate(method='linear', limit_direction='both'),\n",
    "        'overall_mean': lambda df : df.fillna(df.mean()),\n",
    "        'overall_median': lambda df : df.fillna(df.median()),\n",
    "        'backward_fill' : lambda df : df.fillna(method='bfill'),\n",
    "        'forward_fill' : lambda df : df.fillna(method='ffill')\n",
    "    }\n",
    "\n",
    "    \"Class that represents a wandb artifact containing time series data. sd stands for start_date \\\n",
    "    and ed for end_date. Both should be pd.Timestamps\"\n",
    "\n",
    "    @delegates(wandb.Artifact.__init__)\n",
    "    def __init__(self, name, sd:pd.Timestamp, ed:pd.Timestamp, **kwargs):\n",
    "        super().__init__(type='dataset', name=name, **kwargs)\n",
    "        self.sd = sd\n",
    "        self.ed = ed\n",
    "        if self.metadata is None:\n",
    "            self.metadata = dict()\n",
    "        self.metadata['TS'] = dict(sd = self.sd.strftime(self.date_format),\n",
    "                                   ed = self.ed.strftime(self.date_format))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_daily_csv_files(\n",
    "        cls, \n",
    "        root_path, \n",
    "        fread     = pd.read_csv, \n",
    "        start_date= None, \n",
    "        end_date  = None, \n",
    "        metadata  = None, \n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        \"Create a wandb artifact of type `dataset`, containing the CSV files from `start_date` \\\n",
    "        to `end_date`. Dates must be pased as `datetime.datetime` objects. If a `wandb_run` is \\\n",
    "        defined, the created artifact will be logged to that run, using the longwall name as \\\n",
    "        artifact name, and the date range as version.\"\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    @delegates(__init__)\n",
    "    def from_df(\n",
    "        cls, \n",
    "        df:pd.DataFrame, \n",
    "        name:str, \n",
    "        path:str=None, \n",
    "        sd:pd.Timestamp=None, \n",
    "        ed:pd.Timestamp=None,\n",
    "        normalize:bool=False, \n",
    "        missing_values_technique:str=None, \n",
    "        resampling_freq:str=None, \n",
    "        verbose:int = 0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Create a TSArtifact of type `dataset`, using the DataFrame `df` samples from \\\n",
    "        `sd` (start date) to `ed` (end date). Dates must be passed as `datetime.datetime` \\\n",
    "        objects. The transformed DataFrame is stored as a pickle file in the path `path` \\\n",
    "        and its reference is added to the artifact entries. Additionally, the dataset can \\\n",
    "        be normalized (see `normalize` argument) or transformed using missing values \\\n",
    "        handling techniques (see `missing_values_technique` argument) or resampling (see \\\n",
    "        `resampling_freq` argument).\n",
    "\n",
    "        Arguments:\n",
    "            df: (DataFrame) The dataframe you want to convert into an artifact.\n",
    "            name: (str) The artifact name.\n",
    "            path: (str, optional) The path where the file, containing the new transformed \\\n",
    "                dataframe, is saved. Default None.\n",
    "            sd: (sd, optional) Start date. By default, the first index of `df` is taken.\n",
    "            ed: (ed, optional) End date. By default, the last index of `df` is taken.\n",
    "            normalize: (bool, optional) If the dataset values should be normalized. Default\\\n",
    "                False.\n",
    "            missing_values_technique: (str, optional) The technique used to handle missing \\\n",
    "                values. Options: \"linear_iterpolation\", \"overall_mean\", \"overall_median\" or \\\n",
    "                None. Default None.\n",
    "            resampling_freq: (str, optional) The offset string or object representing \\\n",
    "                frequency conversion for time series resampling. Default None.\n",
    "\n",
    "        Returns:\n",
    "            TSArtifact object.\n",
    "        \"\"\"\n",
    "        sd = df.index[0] if sd is None else sd\n",
    "        ed = df.index[-1] if ed is None else ed\n",
    "        if (verbose > 1): print(f\"[ From df ] sd {sd}, ed {ed}\")\n",
    "        obj = cls(name, sd=sd, ed=ed, **kwargs)\n",
    "        df = df.query('@obj.sd <= index <= @obj.ed')\n",
    "        if (verbose > 1): print(f\"[ From df ] df_query~{df.shape}\")\n",
    "        obj.metadata['TS']['created'] = 'from-df'\n",
    "        obj.metadata['TS']['n_vars'] = df.columns.__len__()\n",
    "\n",
    "        # Handle Missing Values\n",
    "        df = obj.handle_missing_values_techniques[missing_values_technique](df) if missing_values_technique is not None else df\n",
    "        obj.metadata['TS']['handle_missing_values_technique'] = missing_values_technique.__str__()\n",
    "        obj.metadata['TS']['has_missing_values'] = np.any(df.isna().values).__str__()\n",
    "        if (verbose > 1): print(f\"[ From df ] df_missing~{df.shape}\")\n",
    "\n",
    "        # Indexing and Resampling\n",
    "        if resampling_freq: df = df.resample(resampling_freq).mean()\n",
    "        obj.metadata['TS']['n_samples'] = len(df)\n",
    "        obj.metadata['TS']['freq'] = str(df.index.freq)\n",
    "        if (verbose > 1): print(f\"[ From df ] df_resampled~{df.shape}\")\n",
    "\n",
    "        # Time Series Variables\n",
    "        obj.metadata['TS']['vars'] = list(df.columns)\n",
    "\n",
    "        # Normalization - Save the previous means and stds\n",
    "        if normalize:\n",
    "            if (verbose > 1): \n",
    "                print(f\" From df ] Normalize df | Before  mean {np.mean(df.iloc[0])}\")\n",
    "                print(f\"[ From df ] Normalize df | Before std {np.std(df.iloc[0])}\")\n",
    "            obj.metadata['TS']['normalization'] = dict(means = df.describe().loc['mean'].to_dict(),\n",
    "                                                       stds = df.describe().loc['std'].to_dict())\n",
    "            df = normalize_columns(df)\n",
    "            if (verbose > 1): \n",
    "                print(f\"[ From df ] Normalize df | After mean {np.mean(df.iloc[0])}\")\n",
    "                print(f\"[ From df ] Normalize df | After std {np.std(df.iloc[0])}\")\n",
    "\n",
    "        # Hash and save\n",
    "        hash_code = str(pd.util.hash_pandas_object(df).sum()) # str(hash(df.values.tobytes()))\n",
    "        path = obj.default_storage_path/f'{hash_code}' if path is None else Path(path)/f'{hash_code}'\n",
    "        if verbose > 0: print(\"About to write df to \", path)\n",
    "        ft.write_feather(df, path, compression = 'lz4')\n",
    "        #feather.write_dataframe\n",
    "        obj.metadata['TS']['hash'] = hash_code\n",
    "        obj.add_file(str(path))\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m  \u001b[0mnormalize_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mnormalize_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"Normalize columns from `df` to have 0 mean and 1 standard deviation\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/dvats/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?? normalize_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/pandas/util/__init__.py:16: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'TS': {'sd': '2021-01-01 00:00:00',\n",
       "  'ed': '2021-01-01 00:00:29',\n",
       "  'created': 'from-df',\n",
       "  'n_vars': 4,\n",
       "  'handle_missing_values_technique': 'overall_median',\n",
       "  'has_missing_values': 'False',\n",
       "  'n_samples': 6,\n",
       "  'freq': '<5 * Seconds>',\n",
       "  'vars': ['A', 'B', 'C', 'D'],\n",
       "  'normalization': {'means': {'A': 0.2555316597420529,\n",
       "    'B': -0.12497235470265032,\n",
       "    'C': -0.291183431309183,\n",
       "    'D': 0.07750569605871739},\n",
       "   'stds': {'A': 0.20238987289909838,\n",
       "    'B': 0.3366392692093527,\n",
       "    'C': 0.3991914356353963,\n",
       "    'D': 0.4794533289833473}},\n",
       "  'hash': '1078852867447619955'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TSArtifact class TEST\n",
    "\n",
    "# resampling frequency\n",
    "resampling_freq = '5s'\n",
    "# handle missing values technique\n",
    "missing_values_technique='overall_median'\n",
    "\n",
    "# testing dataframe\n",
    "df_test = pd.util.testing.makeMissingDataframe()\n",
    "df_test.index = pd.date_range(start='2021-01-01', periods=len(df_test), freq='s')\n",
    "\n",
    "artifact = TSArtifact.from_df(df_test, \n",
    "                              name='JNK', \n",
    "                              missing_values_technique=missing_values_technique,\n",
    "                              resampling_freq=resampling_freq, \n",
    "                              normalize=True)\n",
    "artifact.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/wandb_artifacts/1078852867447619955\n",
      "                            A         B         C         D\n",
      "2021-01-01 00:00:00 -0.464575  0.704954  1.593355 -0.729402\n",
      "2021-01-01 00:00:05  0.710284 -0.551643 -0.285988 -0.799682\n",
      "2021-01-01 00:00:10  0.494932 -1.541817 -0.522709 -0.512139\n",
      "2021-01-01 00:00:15  0.043393  1.184886  0.198768  0.399018\n",
      "2021-01-01 00:00:20  0.972803 -0.343633  0.397988  1.835637\n",
      "2021-01-01 00:00:25 -1.756837  0.547254 -1.381413 -0.193432\n"
     ]
    }
   ],
   "source": [
    "hash = artifact.metadata['TS']['hash']\n",
    "path = \"../../data/wandb_artifacts/\"+hash\n",
    "print(path)\n",
    "f = ft.read_feather(path)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ft.read_feather(\"/home/macu/data/wandb_artifacts/-2535364569820284064\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, we are interested in working with time series as a dataframe. So we need a function to download the files contained in a `wandb.apis.public.Artifact` object and process them into a TS dataframe. The process of passing from files to dataframe must be different depending on what type of creation method was used to generate the original `TSArtifact`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_df(self:wandb.apis.public.Artifact):\n",
    "    \"Download the files of a saved wandb artifact and process them as a single dataframe. The artifact must \\\n",
    "    come from a call to `run.use_artifact` with a proper wandb run.\"\n",
    "    # The way we have to ensure that the argument comes from a TS arfitact is the metadata\n",
    "    if self.metadata.get('TS') is None:\n",
    "        print(f'ERROR:{self} does not come from a logged TSArtifact')\n",
    "        return None\n",
    "    dir = Path(self.download())\n",
    "    if self.metadata['TS']['created'] == 'from-df':\n",
    "        # Call read_pickle with the single file from dir\n",
    "        #return pd.read_pickle(dir.ls()[0])\n",
    "        return ft.read_feather(dir.ls()[0])\n",
    "    else:\n",
    "        print(\"ERROR: Only from_df method is allowed yet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we can write a method to cast a downloaded wandb artifact (instance from `wandb.apis.public,Artifact`) to a TSArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def to_tsartifact(self:wandb.apis.public.Artifact):\n",
    "    \"Cast an artifact as a TS artifact. The artifact must have been created from one of the \\\n",
    "    class creation methods of the class `TSArtifact`. This is useful to go back to a TSArtifact \\\n",
    "    after downloading an artifact through the wand API\"\n",
    "    return TSArtifact(name=self.digest, #TODO change this\n",
    "                      sd=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      ed=pd.to_datetime(self.metadata['TS']['sd'], format=TSArtifact.date_format),\n",
    "                      description=self.description,\n",
    "                      metadata=self.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inject or infer frequencies in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(pd.to_datetime)\n",
    "def infer_or_inject_freq(df, injected_freq='1s', start_date=None, verbose = 0, **kwargs):\n",
    "    \"\"\"\n",
    "        Infer index frequency. If there's not a proper time index, create fake timestamps,\n",
    "        keeping the desired `injected_freq`. If that is None, set a default one of 1 second.\n",
    "        start_date: the first date of the index (int or string).\n",
    "    \"\"\"\n",
    "    inferred_freq = pd.infer_freq(df.index)\n",
    "    if inferred_freq == 'N':\n",
    "        if injected_freq.endswith('mo'):\n",
    "            months = int(injected_freq[:-2])\n",
    "            freq = f'{months}M'\n",
    "            if verbose > 1: print(f\"df~{df.shape} | freq {freq}\")\n",
    "            start_date = pd.to_datetime(ifnone(start_date, df.index[0]))\n",
    "            new_index = pd.date_range(start=start_date, periods=len(df), freq=freq)\n",
    "            df.index = new_index\n",
    "            if verbose > 1: print(df.shape)\n",
    "            df.index.freq = pd.infer_freq(df.index)\n",
    "        else:\n",
    "            try:\n",
    "                timedelta = pd.to_timedelta(injected_freq)\n",
    "                df.index = pd.to_datetime(ifnone(start_date, 0), **kwargs) + timedelta*df.index\n",
    "                df.index.freq = pd.infer_freq(df.index)\n",
    "            except Exception as e: \n",
    "                print(f\"Infer/Inject freq retry. Error: {e}\")\n",
    "                timedelta = pd.to_timedelta(injected_freq)\n",
    "                start_date = pd.to_datetime(ifnone(start_date, df.index[0]), **kwargs)\n",
    "                new_index = pd.date_range(\n",
    "                    start = start_date, periods = len(df), \n",
    "                    freq = timedelta\n",
    "                )\n",
    "                df.index = new_index\n",
    "                if start_date is not None: df.index += start_date\n",
    "                df.index.freq = pd.infer_freq(df.index)\n",
    "    else:\n",
    "        df.index.freq = inferred_freq\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                     0\n",
       " 1970-01-01 00:00:00  1\n",
       " 1970-01-01 00:00:01  2\n",
       " 1970-01-01 00:00:02  3,\n",
       "                      0\n",
       " 1970-01-01 00:00:00  1\n",
       " 1970-01-01 00:00:02  2\n",
       " 1970-01-01 00:00:04  3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = pd.DataFrame([1, 2, 3])\n",
    "foo = infer_or_inject_freq(foo)\n",
    "bar = infer_or_inject_freq(bar, injected_freq='2s')\n",
    "test_eq(foo.index.freq, '1s')\n",
    "test_eq(bar.index.freq, '2s')\n",
    "foo, bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "foo = pd.DataFrame([1, 2, 3])\n",
    "bar = infer_or_inject_freq(foo, injected_freq='1W', start_date='01/01/2020')\n",
    "baz = infer_or_inject_freq(foo, injected_freq='1W', start_date='2020-01-01', format = '%Y-%m-%d')\n",
    "test_eq(bar, baz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=32768):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    # Check if the file is HTML (incorrect download)\n",
    "    if 'text/html' in response.headers['Content-Type']:\n",
    "        print(\"Failed to download the zip file. The link may require additional permissions or the ID may be incorrect.\")\n",
    "    else:\n",
    "        save_response_content(response, destination)\n",
    "        print(f\"File downloaded as: {destination}\")\n",
    "\n",
    "def zip_contents(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        return zip_file.namelist()\n",
    "\n",
    "\n",
    "def unzip_mat(all_one, zip_path, extract_path, case = '', verbose = 0):\n",
    "    if verbose > 0: print(\"--> Unzip_mat\", all_one, zip_path, extract_path, case, verbose)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        mat_files = [file for file in zip_ref.namelist() if file.endswith('.mat') and not file.startswith('__MACOSX/')]\n",
    "        if verbose > 0: print(mat_files)\n",
    "        if all_one == \"all\":\n",
    "            # Extract\n",
    "            for file in mat_files:\n",
    "                zip_ref.extract(file, extract_path)\n",
    "            return f\"{mat_files} extracted to {extract_path}\"\n",
    "        \n",
    "        elif all_one == \"one\":\n",
    "            if case == \"\":\n",
    "                # Extract first .mat\n",
    "                zip_ref.extract(mat_files[0], extract_path)\n",
    "                return f\"{mat_files[0]} extracted to  {extract_path}\"\n",
    "            else:\n",
    "                # Extract <case>.mat\n",
    "                mat_file = next((file for file in mat_files if case in file), None)\n",
    "                if mat_file:\n",
    "                    zip_ref.extract(mat_file, extract_path)\n",
    "                    return f\"{mat_file} extracted to {extract_path}\"\n",
    "                else:\n",
    "                    return \"None \"+case+\".mat found.\"\n",
    "        else:\n",
    "            return \"First parameter must be 'all' or 'one'.\"\n",
    "        if verbose > 0: print(\"unzip_path -->\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3000/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: /home/macu/data, destination: /home/macu/data/exoDataFig1.zip\n",
      "Failed to download the zip file. The link may require additional permissions or the ID may be incorrect.\n"
     ]
    }
   ],
   "source": [
    "file_id = '1Li9D5YyspA6eQtpGRihOi0y9D85cq4GE'\n",
    "name = 'exoDataFig1'\n",
    "data_path = os.path.expanduser('~/data')\n",
    "destination = os.path.join(data_path, name + '.zip')\n",
    "print(f\"Data: {data_path}, destination: {destination}\")\n",
    "download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download the zip file. The link may require additional permissions or the ID may be incorrect.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/macu/data/InsectData-fig11.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Llamada a la función\u001b[39;00m\n\u001b[1;32m      8\u001b[0m download_file_from_google_drive(file_id, destination)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mzip_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(unzip_mat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, destination,  data_path))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(unzip_mat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone\u001b[39m\u001b[38;5;124m'\u001b[39m, destination,  data_path))\n",
      "Cell \u001b[0;32mIn[15], line 37\u001b[0m, in \u001b[0;36mzip_contents\u001b[0;34m(zip_path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzip_contents\u001b[39m(zip_path):\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_file:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m zip_file\u001b[38;5;241m.\u001b[39mnamelist()\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/zipfile.py:1251\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/macu/data/InsectData-fig11.zip'"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Downloading insects (by Eamonn Keogh)\n",
    "file_id = '1qq1z2mVRd7PzDqX0TDAwY7BcWVjnXUfQ'\n",
    "data_path = os.path.expanduser('~/data')\n",
    "destination = os.path.join(data_path, 'InsectData-fig11.zip')\n",
    "\n",
    "# Llamada a la función\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "zip_contents(destination)\n",
    "print(unzip_mat('all', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_million'))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_millione'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "def mat2csv(mat_file_path, csv_file_folder = '~/data/', verbose = 0):\n",
    "    # Carga el archivo .mat, omitiendo las variables meta de MATLAB\n",
    "    mat = scipy.io.loadmat(mat_file_path, squeeze_me=True, struct_as_record=False)\n",
    "    \n",
    "    # Itera sobre todas las variables encontradas en el archivo .mat\n",
    "    for variable_name, data in mat.items():\n",
    "        if variable_name.startswith('__') or isinstance(data, scipy.io.matlab.mio5_params.mat_struct):\n",
    "            # Omite variables meta de MATLAB o estructuras (que requieren un manejo especial)\n",
    "            continue\n",
    "        \n",
    "        # Convierte la data a un DataFrame de pandas, manejando diferentes tipos de datos\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.dtype.names:  # Es un ndarray estructurado\n",
    "                data_df = pd.DataFrame(data)\n",
    "            else:  # Es un ndarray regular\n",
    "                data_df = pd.DataFrame(data, columns=[variable_name])\n",
    "        else:\n",
    "            # Para otros tipos de datos, los convertimos en un DataFrame simple\n",
    "            data_df = pd.DataFrame([data], columns=[variable_name])\n",
    "        \n",
    "        # Define la ruta del archivo .csv de salida\n",
    "        csv_file_path = csv_file_folder+ variable_name + '.csv'\n",
    "        \n",
    "        # Guarda el DataFrame como un archivo .csv\n",
    "        data_df.to_csv(csv_file_path, index=False)\n",
    "        if verbose > 0:\n",
    "            print(data_df.shape)\n",
    "            display(data_df.head(5))\n",
    "            print(f\"Matlab matrix '{variable_name}' converted to CSV in: {csv_file_path}\")\n",
    "        return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path = '/home/macu/data/MP_first_test_penguin_sample.mat'\n",
    "mat2csv(path, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path = '/home/macu/data/Insect_one_million.mat'\n",
    "mat2csv(path, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            return value\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=32768):\n",
    "            if chunk:  # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    # Check if the file is HTML (incorrect download)\n",
    "    if 'text/html' in response.headers['Content-Type']:\n",
    "        print(\"Failed to download the zip file. The link may require additional permissions or the ID may be incorrect.\")\n",
    "    else:\n",
    "        save_response_content(response, destination)\n",
    "        print(f\"File downloaded as: {destination}\")\n",
    "\n",
    "def zip_contents(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_file:\n",
    "        return zip_file.namelist()\n",
    "\n",
    "\n",
    "def unzip_mat(all_one, zip_path, extract_path, case = '', verbose = 0):\n",
    "    if verbose > 0: print(\"--> Unzip_mat\", all_one, zip_path, extract_path, case, verbose)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        mat_files = [file for file in zip_ref.namelist() if file.endswith('.mat') and not file.startswith('__MACOSX/')]\n",
    "        if verbose > 0: print(mat_files)\n",
    "        if all_one == \"all\":\n",
    "            # Extract\n",
    "            for file in mat_files:\n",
    "                zip_ref.extract(file, extract_path)\n",
    "            return f\"{mat_files} extracted to {extract_path}\"\n",
    "        \n",
    "        elif all_one == \"one\":\n",
    "            if case == \"\":\n",
    "                # Extract first .mat\n",
    "                zip_ref.extract(mat_files[0], extract_path)\n",
    "                return f\"{mat_files[0]} extracted to  {extract_path}\"\n",
    "            else:\n",
    "                # Extract <case>.mat\n",
    "                mat_file = next((file for file in mat_files if case in file), None)\n",
    "                if mat_file:\n",
    "                    zip_ref.extract(mat_file, extract_path)\n",
    "                    return f\"{mat_file} extracted to {extract_path}\"\n",
    "                else:\n",
    "                    return \"None \"+case+\".mat found.\"\n",
    "        else:\n",
    "            return \"First parameter must be 'all' or 'one'.\"\n",
    "        if verbose > 0: print(\"unzip_path -->\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3000/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1Li9D5YyspA6eQtpGRihOi0y9D85cq4GE'\n",
    "name = 'exoDataFig1'\n",
    "data_path = os.path.expanduser('~/data')\n",
    "destination = os.path.join(data_path, name + '.zip')\n",
    "print(f\"Data: {data_path}, destination: {destination}\")\n",
    "download_file_from_google_drive(file_id, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Downloading insects (by Eamonn Keogh)\n",
    "file_id = '1qq1z2mVRd7PzDqX0TDAwY7BcWVjnXUfQ'\n",
    "data_path = os.path.expanduser('~/data')\n",
    "destination = os.path.join(data_path, 'InsectData-fig11.zip')\n",
    "\n",
    "# Llamada a la función\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "zip_contents(destination)\n",
    "print(unzip_mat('all', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_million'))\n",
    "print(unzip_mat('one', destination,  data_path, 'Insect_one_millione'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#from nbdev.export import *\n",
    "#notebook2script()\n",
    "beep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
