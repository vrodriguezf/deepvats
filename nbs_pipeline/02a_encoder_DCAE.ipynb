{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplay(obj)\n\u001b[0;32m----> 6\u001b[0m vs\u001b[39m.\u001b[39mDisplayHandle\u001b[39m.\u001b[39mupdate \u001b[39m=\u001b[39m update_patch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vs' is not defined"
     ]
    }
   ],
   "source": [
    "# This is only needed if the notebook is run in VSCode\n",
    "from IPython.display import clear_output, DisplayHandle\n",
    "def update_patch(self, obj):\n",
    "    clear_output(wait=True)\n",
    "    self.display(obj)\n",
    "DisplayHandle.update = update_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DCAE: Deep Convolutional Autoencoder\n",
    "\n",
    "> This notebook encodes the data using the autoencoder described in [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tsai.all import *\n",
    "except:\n",
    "    from tsai.all import * # TODO: Weird error when loading tsai!from tchub.all import *\n",
    "import wandb\n",
    "wandb_api = wandb.Api()\n",
    "from fastcore.all import *\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import *\n",
    "from dvats.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "Before running this notebook, make sure you have the `$WANDB_API_KEY` environment varibale defined with your API_KEY (run in a terminal `echo $WANDB_API_KEY` to see it). If not, run in a terminal `wandb login [API_KEY]`. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AttrDict(\n",
    "    use_wandb = True, # Whether to use or not wandb for experiment tracking\n",
    "    wandb_group = None, # Whether to group this run in a wandb group\n",
    "    wandb_entity = 'vrodriguezf90',\n",
    "    wandb_project = 'deepvats',\n",
    "    train_artifact = 'vrodriguezf90/deepvats/toy:v0', # complete name\n",
    "    valid_artifact = None, # complete name (None for random validation set)\n",
    "    valid_size = 0.1,  # In case valid_artifact is None, this will set the percentage of random items to go to val\n",
    "    w = 32, # window size for the sliding window\n",
    "    stride = 1, # n datapoints the window is moved ahead along the sequence in the sliding window\n",
    "    delta = 60, # Size of the autoencoder bottleneck layer\n",
    "    nfs = [64,32,16], #The number of filters in the last conv layer must be equal to the product of pool sizes\n",
    "    kss = [10,5,5],\n",
    "    output_filter_size = 10,\n",
    "    pool_szs = [2, 2, 4], # The product of pool sizes must be a divisor of the window size\n",
    "    batch_size = 64,\n",
    "    epochs = 200,\n",
    "    top_k = 3 # Number of elements to analyse for the top losses\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: deepvats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: project not found (<Response [404]>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9675ada41b4d26a2993de490bb3b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666959107387811, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_442920/1553872374.py 2 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwandb_project)\n\u001b[0;32m----> 2\u001b[0m run \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_entity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_project\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mencoder_DCAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_val_change\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monline\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdisabled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m config \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39mconfig  \u001b[38;5;66;03m# Object for storing hyperparameters\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Botch to use artifacts offline\u001b[39;00m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1168\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[1;32m   1167\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[0;32m-> 1168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1170\u001b[0m     error_seen \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1145\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1143\u001b[0m except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1145\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:735\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunicating run to backend with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m second timeout\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    734\u001b[0m run_init_handle \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_run(run_proto)\n\u001b[0;32m--> 735\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_init_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcancel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    741\u001b[0m     run_result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mrun_result\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_and_clear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/env/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/env/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Project: \"+config.wandb_project)\n",
    "run = wandb.init(\n",
    "    entity = config.wandb_entity,\n",
    "    project=config.wandb_project,\n",
    "    group=config.wandb_group,\n",
    "    job_type='encoder_DCAE',\n",
    "    allow_val_change=True,\n",
    "    mode='online' if config.use_wandb else 'disabled',\n",
    "    config=config,\n",
    "    resume=False\n",
    ")\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "# Botch to use artifacts offline\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets\n",
    "\n",
    "To load the dataset we will download a specific dataset artifact from the collection of artifacts\n",
    "stored in the weights and biases (wandb) project associated to this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a continuous multivariate time-series data $D$ of dimension $d$ with $n$ time-steps, $D = X_1,X_2,\\dots,X_n$ , where each $X_i = \\{x_i^1,\\dots,x_i^d\\}$ . Let $w$ be the window width, $s$ the stride, and $t$ the start time of a sliding window in the data.\n",
    "\n",
    "Define a new matrix $Z_k$ where each row is a vector of size $w$ of data extracted from the $k^{th}$ dimension.\n",
    "\n",
    "\\begin{aligned}&Z_k(w,s,t)\\\\&\\quad =\\begin{bmatrix} x_{t}^k&\\quad x_{t+1}^k&\\quad \\dots&\\quad x_{t+w-1}^k \\\\ x_{t+s}^k&\\quad x_{t+s+1}^k&\\quad \\dots&\\quad x_{t+s+w-1}^k \\\\ \\vdots&\\quad \\vdots&\\quad \\ddots&\\quad \\vdots \\\\ x_{t+(r-1)s}^k&\\quad x_{t+(r-1)s+1}^k&\\quad \\dots&\\quad x_{t+(r-1)s+w-1}^k \\end{bmatrix} \\end{aligned}\n",
    "\n",
    "where $r$ is the number of desired rows, and $t+(r-1)s+w-1 \\le n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$ is a $w \\times s \\times t$ matrix. The first step consists in slicing the original multivariate time series into slices of shape ($w \\times d$), as shown in this figure from the paper.\n",
    "<img src=\"https://i.imgur.com/R9Fx8uO.png\" style=\"width:800px;height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of this sliding window approach are given values by default here. If the value has been already set previously, that means this notebook is being called from a wandb sweep, and we must use the value that the sweep is bringing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SlidingWindow(window_len=config.w, stride=config.stride, get_y=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_artifact = artifacts_gettr(config.train_artifact)\n",
    "df_train = train_artifact.to_df()\n",
    "X_train, _ = sw(df_train)\n",
    "df_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.valid_artifact:\n",
    "    valid_artifact = artifacts_gettr(config.valid_artifact)\n",
    "    df_val = valid_artifact.to_df()\n",
    "    X_valid, _ = sw(df_val)\n",
    "    df_val.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important features from the multivariate time series data through Deep Convolutional Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional Auto Encoders (DCAE) is a powerful method for learning high-level and mid-level abstractions from low-level raw data. It has the ability to extract features from complex and large time-series in an unsupervised manner. This is useful to overcome the complexity of multivariate time-series.\n",
    "\n",
    "Compared to the conventional auto-encoder, DCAE has fewer parameters than the conventional auto-encoder which means less training time. Also, DCAE uses local information to reconstruct the signal while conventional auto-encoders utilize fully connected layers to globally do the reconstruction. DCAE is an unsupervised model for representation learning which maps inputs into a new representation space. It has two main parts which are the encoding part that is used to project the data into a set of feature spaces and the decoding part that reconstructs the original data. The latent space representation is the space where the data lie in the bottleneck layers.\n",
    "\n",
    "The loss function of the DCAE is defined as the error between the input and the output. DCAE aims to find a code for each input by minimizing the mean squared error (MSE) between its input (original data) and output (reconstructed data). The MSE is used which assists to minimize the loss; thus, the network is forced to learn a low-dimensional representation of the input.\n",
    "\n",
    "We will implement the DCAE of the paper [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y), whose architecture is shown in the table below:\n",
    "\n",
    "![](https://i.imgur.com/3EjuAfQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the paper, the input shape is $60 \\times 3$, due to multivariate time series has 3 variables and the window size is 60. Generally, the size of the input/output of the autoencoder will depend on the shape of each slice obtained in the previos step. The number of latent features to be discovered is $60$ in the table above, but we can consider this as a free hyperparameter $\\delta$. Also, according to the paper: \"*The number of feature maps, size of filter and depth of the model are set based on the reconstruction error on validation set.*\". Thus, we must provide flexibility in the creation of the DCAE in terms of these hyperparameters.º"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are not using a config file, you can also uncomment the following cell and define the hyperparameters in the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_equal([len(x) for x in [config.nfs, config.kss, config.pool_szs]], \n",
    "          np.repeat(len(config.nfs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The implementation of the DCAE is done using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DCAE_torch(c_in=X_train.shape[1], seq_len=config.w, delta=config.delta, \n",
    "               pool_szs=config.pool_szs, nfs=config.nfs)\n",
    "print(m)\n",
    "foo = torch.rand(1, X_train.shape[1], config.w)\n",
    "foo.shape, m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with fastai Learner class, to abstract from Pytorch's training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.valid_artifact:\n",
    "    X, y, splits  = combine_split_data(xs=[X_train, X_valid], ys=[X_train, X_valid])\n",
    "else:\n",
    "    X = X_train\n",
    "    y = X_train\n",
    "    splits = get_splits(np.arange(len(X)), valid_size=config.valid_size)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [ToFloat(), ToFloat()]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "dls.dataset[0][0].show(), ToTSTensor()(dls.dataset[0][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DCAE_torch(c_in=X_train.shape[1], seq_len=config.w, delta=config.delta, \n",
    "               pool_szs=config.pool_szs, nfs=config.nfs)\n",
    "learn = Learner(dls=dls, model=m, loss_func=nn.MSELoss(), opt_func=Adam, \n",
    "                cbs=[WandbCallback(log_preds=False)])\n",
    "lr_valley, lr_steep = learn.lr_find(suggest_funcs=[valley, steep])\n",
    "learn.fit_one_cycle(config.epochs, lr_max=lr_valley)\n",
    "learn.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "To track the performance of this model fit, go to the project dashboard in Weights & Biases. The link is provided at the beginning of this notebook, after the execution of the function `wandb.init()'' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, log the learner to be used by the next notebook in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_learn = learn.export_and_get()\n",
    "if config.use_wandb: \n",
    "    ar = ReferenceArtifact(aux_learn, f'dcae', type='learner', metadata=dict(run.config))\n",
    "    run.log_artifact(ar, aliases=f'run-{run.project}-{run.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline models (To rewrite)\n",
    "\n",
    "Calculate baseline models taking into account that the best prediction is the average and median value of each of the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the autoencoder\n",
    "\n",
    "Let's validate the autoencoder quality visually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best and the worst k predictions using the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses = interp.top_losses(3)\n",
    "top_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in top_losses.indices: dls.dataset[i][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
