{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab58124-8fe4-4494-8c7a-b463427d3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation\n",
    "\n",
    "# Clone the ibm/tsfm\n",
    "#! git clone https://github.com/ibm-granite/granite-tsfm.git\n",
    "#! ls\n",
    "\n",
    "# Change directory. Move inside the tsfm repo.\n",
    "#%cd granite-tsfm\n",
    "#! ls\n",
    "\n",
    "# Relax requirement for python version < 3.12\n",
    "#! sed -i.orig 's/3\\.12/3.13/g' pyproject.toml\n",
    "\n",
    "# Install the tsfm library\n",
    "#! pip install \".[notebooks]\"\n",
    "#! python3 -m pip install \".[notebooks]\"\n",
    "#! pip3 install \".[notebooks]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355e8fd8-7aca-43d8-bcf6-573f0f2fbf02",
   "metadata": {},
   "source": [
    "NO SE SELECCIONA PORQUE LA LONGITUD DE LA SECUENCIA DE ENTRADA ESTÃ PREFIJADA A DOS VALORES => NO HAY SUFICIENTE FLEXIBILIDAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28f059b6-d67d-4892-901d-fb7dab810eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd21735b-b5a8-4c26-be6e-344f6f015752",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.expanduser(\"~/work/nbs_pipeline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f2a01f-0b75-45ff-9a79-77f2b514df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "verbose                       = 0\n",
    "check_memory_usage            = True\n",
    "time_flag                     = True\n",
    "window_size_percentage        = True\n",
    "show_plots                    = True\n",
    "reset_kernel                  = True\n",
    "pre_configured_case           = True\n",
    "case_id                       = 7\n",
    "frequency_factor              = 1\n",
    "frequency_factor_change_alias = True\n",
    "check_parameters              = True\n",
    "cuda_device                   = 0\n",
    "remove_lambdas_flag           = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88f5ea2-ad6f-4f7f-9b97-7a7bab91eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 72 from C header, got 88 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.IpcReadOptions size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.Tensor size changed, may indicate binary incompatibility. Expected 64 from C header, got 80 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.NativeFile size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.BufferedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedInputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: pyarrow.lib.CompressedOutputStream size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004l\n",
      "Octave is ready <oct2py.core.Oct2Py object at 0x7fa8ec748ee0>\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004l\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "import sys\n",
    "import dvats.utils as ut\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import uni2ts\n",
    "from uni2ts.eval_util.plot import plot_single\n",
    "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
    "from uni2ts.eval_util.plot import plot_next_multi\n",
    "import pyarrow.feather as ft\n",
    "from gluonts.transform.split import TFTInstanceSplitter\n",
    "from gluonts.transform.sampler import TestSplitSampler\n",
    "import numpy as np\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "from dvats.memory import gpu_memory_status\n",
    "import dvats.config as cfg_\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"umap\")\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from dvats.all import *\n",
    "from fastcore.all import *\n",
    "from tsai.basics import *\n",
    "from tsai.models.InceptionTimePlus import *\n",
    "from tsai.callback.MVP import *\n",
    "import matplotlib.colors as colors\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.progress import ShowGraphCallback\n",
    "from fastai.callback.schedule import *\n",
    "from fastai.callback.tracker import EarlyStoppingCallback\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a245721e-ac89-44c4-8126-c12ed31381d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfm_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f87f50fd-b7cb-41c8-a817-4b313035a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_mixer_for_prediction = tsfm_public.TinyTimeMixerForPrediction.from_pretrained(\n",
    "            \"ibm/TTM\", revision=\"1024_96_v1\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e8d29f-104b-42ad-8ad7-6156716f50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_learner = tiny_mixer_for_prediction.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ded22c6-d988-40bf-8346-dea80035b9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of TinyTimeMixerModel(\n",
       "  (encoder): TinyTimeMixerEncoder(\n",
       "    (patcher): Linear(in_features=64, out_features=192, bias=True)\n",
       "    (mlp_mixer_encoder): TinyTimeMixerBlock(\n",
       "      (mixers): ModuleList(\n",
       "        (0): TinyTimeMixerAdaptivePatchingBlock(\n",
       "          (mixer_layers): ModuleList(\n",
       "            (0-1): 2 x TinyTimeMixerLayer(\n",
       "              (patch_mixer): PatchMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "              (feature_mixer): FeatureMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=48, out_features=96, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=96, out_features=48, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): TinyTimeMixerAdaptivePatchingBlock(\n",
       "          (mixer_layers): ModuleList(\n",
       "            (0-1): 2 x TinyTimeMixerLayer(\n",
       "              (patch_mixer): PatchMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=32, out_features=32, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "              (feature_mixer): FeatureMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=96, out_features=192, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=192, out_features=96, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): TinyTimeMixerAdaptivePatchingBlock(\n",
       "          (mixer_layers): ModuleList(\n",
       "            (0-1): 2 x TinyTimeMixerLayer(\n",
       "              (patch_mixer): PatchMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=16, out_features=32, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=16, out_features=16, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "              (feature_mixer): FeatureMixerBlock(\n",
       "                (norm): TinyTimeMixerNormLayer(\n",
       "                  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "                (mlp): TinyTimeMixerMLP(\n",
       "                  (fc1): Linear(in_features=192, out_features=384, bias=True)\n",
       "                  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "                  (fc2): Linear(in_features=384, out_features=192, bias=True)\n",
       "                  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "                )\n",
       "                (gating_block): TinyTimeMixerGatedAttention(\n",
       "                  (attn_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (attn_softmax): Softmax(dim=-1)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (patching): TinyTimeMixerPatchify()\n",
       "  (scaler): TinyTimeMixerStdScaler()\n",
       ")>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_learner.named_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b56e3c-7b1c-427e-8282-db1620119234",
   "metadata": {},
   "source": [
    "### Averiguando la capa interesante del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21e598f8-e93b-4fbf-abbe-9dc4165324f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_modules of Softmax(dim=-1)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = enc_learner.encoder.mlp_mixer_encoder.mixers[2].mixer_layers[1].feature_mixer.gating_block.attn_softmax\n",
    "module.named_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1031f-8bfc-439f-90cf-be21989e3a29",
   "metadata": {},
   "source": [
    "### Construyendo el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "069304f3-7959-4d0c-9adb-c8155bed8ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find /home/macu/work/nbs_pipeline/02c_encoder_moment-embedding.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmi-santamaria\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macu/work/wandb/run-20241008_142759-716i722q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mi-santamaria/deepvats/runs/716i722q' target=\"_blank\">02c_encoder_moment-embedding</a></strong> to <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mi-santamaria/deepvats' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mi-santamaria/deepvats/runs/716i722q' target=\"_blank\">https://wandb.ai/mi-santamaria/deepvats/runs/716i722q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train ~ (550, 3)\n",
      "(550, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T3</th>\n",
       "      <th>T2</th>\n",
       "      <th>T1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:00</th>\n",
       "      <td>0.741822</td>\n",
       "      <td>0.637180</td>\n",
       "      <td>0.565117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:01</th>\n",
       "      <td>0.739731</td>\n",
       "      <td>0.629415</td>\n",
       "      <td>0.493513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:02</th>\n",
       "      <td>0.718757</td>\n",
       "      <td>0.539220</td>\n",
       "      <td>0.469350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:03</th>\n",
       "      <td>0.730169</td>\n",
       "      <td>0.577670</td>\n",
       "      <td>0.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-01 00:00:04</th>\n",
       "      <td>0.752406</td>\n",
       "      <td>0.570180</td>\n",
       "      <td>0.373008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           T3        T2        T1\n",
       "1970-01-01 00:00:00  0.741822  0.637180  0.565117\n",
       "1970-01-01 00:00:01  0.739731  0.629415  0.493513\n",
       "1970-01-01 00:00:02  0.718757  0.539220  0.469350\n",
       "1970-01-01 00:00:03  0.730169  0.577670  0.444100\n",
       "1970-01-01 00:00:04  0.752406  0.570180  0.373008"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train ~ (521, 3, 30)\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "user, project, version, data, config, job_type = cfg_.get_artifact_config_MVP(False)\n",
    "if pre_configured_case: \n",
    "    cfg_.force_artifact_config_mvp(\n",
    "        config = config,\n",
    "        id = case_id,\n",
    "        verbose = verbose, \n",
    "        both = verbose > 0,\n",
    "        frequency_factor = frequency_factor,\n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )\n",
    "#| export\n",
    "path = os.path.expanduser(\"~/work/nbs_pipeline/\")\n",
    "name=\"02c_encoder_moment-embedding\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = path+name+\".ipynb\"\n",
    "runname=name\n",
    "if verbose > 0: print(\"runname: \"+runname)\n",
    "if verbose > 0: cfg_.show_attrdict(config)\n",
    "\n",
    "#| export\n",
    "if verbose > 0: print(\"--> Wandb init\")\n",
    "run = wandb.init(\n",
    "    entity = user,\n",
    "    # work-nbs is a place to log draft runs\n",
    "    project=project,\n",
    "    group=config.wandb_group,\n",
    "    job_type=job_type,\n",
    "    allow_val_change=True,\n",
    "    mode=config.analysis_mode,\n",
    "    config=config,\n",
    "    # When use_wandb is false the run is not linked to a personal account\n",
    "    #NOTE: This is not working right now\n",
    "    anonymous = 'never' if config.use_wandb else 'must', \n",
    "    resume=False,\n",
    "    name = runname\n",
    ")\n",
    "if verbose > 0: print(\"Wandb init -->\")\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact\n",
    "#| export\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "if verbose > 0: cfg_.show_attrdict(config)\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact\n",
    "train_artifact = artifacts_gettr(config.train_artifact)\n",
    "if verbose > 0: print(\"---> W&B Train Artifact\")\n",
    "\n",
    "#| export\n",
    "import pyarrow.feather as ft\n",
    "df_train = train_artifact.to_df()\n",
    "print(\"df_train ~\", df_train.shape)\n",
    "print(df_train.shape)\n",
    "display(df_train.head())\n",
    "#| export\n",
    "if verbose > 0: print(\"---> Sliding window | \", config.w,  \" | \", config.stride )\n",
    "sw = SlidingWindow(window_len=config.w, stride=config.stride, get_y=[])\n",
    "if verbose > 0: print(\" Sliding window | \", config.w,  \" | \", config.stride, \"---> | df_train ~ \", df_train.shape )\n",
    "X_train, _ = sw(df_train)\n",
    "if verbose > 0: print(\" sw_df_train | \", config.w,  \" | \", config.stride, \"--->\" )\n",
    "print(\"X_train ~\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825791e7-d8c1-4f40-aaf2-008503e0caf0",
   "metadata": {},
   "source": [
    "### Adaptando la llamada al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32fe1ada-8f34-4db3-a7a2-b6aa68b84fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       " \u001b[0menc_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpast_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpast_observed_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfreq_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtsfm_public\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtinytimemixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_tinytimemixer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTinyTimeMixerModelOutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The [`TinyTimeMixerModel`] forward method, overrides the `__call__` special method.\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Although the recipe for forward pass needs to be defined within this function, one should call the [`Module`]\n",
       "instance afterwards instead of this since the former takes care of running the pre and post processing steps while\n",
       "the latter silently ignores them.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Args:\n",
       "    past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n",
       "        Context values of the time series. For a forecasting task, this denotes the history/past time series values.\n",
       "        For univariate time series, `num_input_channels` dimension should be 1. For multivariate time series, it is\n",
       "        greater than 1.\n",
       "\n",
       "    output_hidden_states (`bool`, *optional*):\n",
       "        Whether or not to return the hidden states of all layers.\n",
       "\n",
       "    return_dict (`bool`, *optional*):\n",
       "        Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
       "\n",
       "    past_observed_mask (`torch.Tensor` of shape `(batch_size, sequence_length, num_input_channels)`, *optional*):\n",
       "        Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected\n",
       "        in `[0, 1]` or `[False, True]`:\n",
       "            - 1 or True for values that are **observed**,\n",
       "            - 0 or False for values that are **missing** (i.e. NaNs that were replaced by zeros).\n",
       "\n",
       "\n",
       "    Returns:\n",
       "        [`tsfm_public.models.tinytimemixer.modeling_tinytimemixer.TinyTimeMixerModelOutput`] or `tuple(torch.FloatTensor)`: A [`tsfm_public.models.tinytimemixer.modeling_tinytimemixer.TinyTimeMixerModelOutput`] or a tuple of\n",
       "        `torch.FloatTensor` (if `return_dict=False` is passed or when `config.return_dict=False`) comprising various\n",
       "        elements depending on the configuration ([`TinyTimeMixerConfig`]) and inputs.\n",
       "\n",
       "        - **last_hidden_state** (`torch.FloatTensor`  of shape `(batch_size, num_channels, num_patches, d_model)`) -- Hidden-state at the output of the last layer of the model.\n",
       "        - **hidden_states** (`tuple(torch.FloatTensor)`, *optional*) -- Hidden-states of the model at the output of each layer.\n",
       "        - **patch_input** (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`) -- Patched input data to the model.\n",
       "        - **loc:** (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*) -- Gives the mean of the context window per channel. Used for revin denorm outside the model, if revin\n",
       "          enabled.\n",
       "        - **scale:** (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*) -- Gives the std dev of the context window per channel. Used for revin denorm outside the model, if revin\n",
       "          enabled.\n",
       "  \n",
       "\n",
       "    \n",
       "\u001b[0;31mFile:\u001b[0m      ~/work/nbs_pipeline/granite-tsfm/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? enc_learner.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5518cc00-95bd-4d72-bdb3-178c7fa9b221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521, 3, 30)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afbb8b33-afb6-4042-ba73-702e7d4b6a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_values = einops.rearrange(\n",
    "    torch.from_numpy(X_train),\n",
    "    \"num_windows num_vars window_len -> num_windows window_len num_vars\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480ff3c-67f5-46e5-ae6e-927ca6cd221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = enc_learner.encoder.mlp_mixer_encoder.mixers[2].mixer_layers[1].feature_mixer.gating_block.attn_softmax\n",
    "module.named_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef003168-1451-4439-b047-b68d44903076",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input sequence length (30) doesn't match model configuration (1024).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acts \u001b[38;5;241m=\u001b[39m \u001b[43mget_acts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menc_learner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_values\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/dvats/encoder.py:92\u001b[0m, in \u001b[0;36mget_acts\u001b[0;34m(model, module, cpu, **model_kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     91\u001b[0m h_act \u001b[38;5;241m=\u001b[39m hook_outputs([module], detach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, cpu \u001b[38;5;241m=\u001b[39m cpu, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 92\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [o\u001b[38;5;241m.\u001b[39mstored \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m h_act]\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nbs_pipeline/granite-tsfm/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:1585\u001b[0m, in \u001b[0;36mTinyTimeMixerModel.forward\u001b[0;34m(self, past_values, past_observed_mask, output_hidden_states, return_dict, freq_token)\u001b[0m\n\u001b[1;32m   1582\u001b[0m     past_observed_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(past_values)\n\u001b[1;32m   1583\u001b[0m scaled_past_values, loc, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler(past_values, past_observed_mask)\n\u001b[0;32m-> 1585\u001b[0m patched_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_past_values\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size x num_input_channels x num_patch x patch_length\u001b[39;00m\n\u001b[1;32m   1587\u001b[0m enc_input \u001b[38;5;241m=\u001b[39m patched_x\n\u001b[1;32m   1589\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1590\u001b[0m     enc_input,\n\u001b[1;32m   1591\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1592\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1593\u001b[0m     freq_token\u001b[38;5;241m=\u001b[39mfreq_token,\n\u001b[1;32m   1594\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/share/miniconda3/envs/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/nbs_pipeline/granite-tsfm/tsfm_public/models/tinytimemixer/modeling_tinytimemixer.py:1254\u001b[0m, in \u001b[0;36mTinyTimeMixerPatchify.forward\u001b[0;34m(self, past_values)\u001b[0m\n\u001b[1;32m   1252\u001b[0m sequence_length \u001b[38;5;241m=\u001b[39m past_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequence_length \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length:\n\u001b[0;32m-> 1254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput sequence length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model configuration (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1256\u001b[0m     )\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;66;03m# output: [bs x new_sequence_length x num_channels]\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m output \u001b[38;5;241m=\u001b[39m past_values[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_start :, :]\n",
      "\u001b[0;31mValueError\u001b[0m: Input sequence length (30) doesn't match model configuration (1024)."
     ]
    }
   ],
   "source": [
    "acts = get_acts(\n",
    "    model = enc_learner,\n",
    "    module = module,\n",
    "    cpu = False,\n",
    "    past_values = past_values\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
