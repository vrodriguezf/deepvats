{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if the notebook is run in VSCode\n",
    "import nbs_pipeline.utils.vscode  as vs\n",
    "vs.DisplayHandle.update = vs.update_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# DCAE: Deep Convolutional Autoencoder\n",
    "\n",
    "> This notebook encodes the data using the autoencoder described in [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tsai.all import *\n",
    "except:\n",
    "    from tsai.all import * # TODO: Weird error when loading tsai!from tchub.all import *\n",
    "import wandb\n",
    "wandb_api = wandb.Api()\n",
    "from fastcore.all import *\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "from fastai.callback.schedule import *\n",
    "from dvats.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the experiment tracking and hyperparameter we will use the tool **Weights & Biases**. \n",
    "\n",
    "Before running this notebook, make sure you have the `$WANDB_API_KEY` environment varibale defined with your API_KEY (run in a terminal `echo $WANDB_API_KEY` to see it). If not, run in a terminal `wandb login [API_KEY]`. You can see your API_KEY [here](https://wandb.ai/authorize) or in the settings of your W&B account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current: /home/macu/work/nbs_pipeline\n",
      "yml: ./config/02b-encoder_dcae.yaml\n",
      "... About to replace includes with content\n",
      "Before configuration reading \n",
      "-include: None\n",
      "-user_preferences:\n",
      "\t-use_wandb: False\n",
      "\t-wdb:\n",
      "\t\t-user: mi-santamaria\n",
      "\t\t-project_name: test-project\n",
      "\t\t-version: 0\n",
      "\t\t-mode: offline\n",
      "\t\t-artifacts_path: ./data/wandb_artifacts\n",
      "\t-data:\n",
      "\t\t-folder: ~/data/\n",
      "\t\t-fname: speed_6005\n",
      "\t\t-ftype: .csv\n",
      "\t\t-cols: [1]\n",
      "\t\t-freq: 1s\n",
      "\t-artifact:\n",
      "\t\t-alias: TiltABP\n",
      "\t-directories:\n",
      "\t\t-tmp: tmp\n",
      "\t\t-data: ~/data/speed_6005.csv\n",
      "-data:\n",
      "\t-name: speed_6005\n",
      "\t-path: ~/data/speed_6005.csv\n",
      "\t-alias: TiltABP\n",
      "\t-cols: [1]\n",
      "\t-csv_config:\n",
      "\t-date_offset: None\n",
      "\t-date_format: %Y-%m-%d %H:%M:%S\n",
      "\t-freq: 1s\n",
      "\t-joining_train_test: False\n",
      "\t-missing_values:\n",
      "\t\t-technique: None\n",
      "\t\t-constant: None\n",
      "\t-normalize_training: False\n",
      "\t-range_training: None\n",
      "\t-range_testing: None\n",
      "\t-resampling_freq: None\n",
      "\t-start_date: None\n",
      "\t-test_split: None\n",
      "\t-time_col: None\n",
      "-wandb:\n",
      "\t-user: mi-santamaria\n",
      "\t-dir: ~/test-project\n",
      "\t-enabled: False\n",
      "\t-group: None\n",
      "\t-log_learner: False\n",
      "\t-mode: offline\n",
      "\t-project: test-project\n",
      "\t-version: 0\n",
      "\t-artifacts_path: ./data/wandb_artifacts\n",
      "-configuration:\n",
      "\t-job_type: encoder_DCAE\n",
      "\t-alias: TiltABP\n",
      "\t-wandb:\n",
      "\t\t-use: False\n",
      "\t\t-entity: mi-santamaria\n",
      "\t\t-group: None\n",
      "\t\t-project: test-project\n",
      "\t-artifacts:\n",
      "\t\t-train: mi-santamaria/test-project/speed_6005:v0\n",
      "\t\t-valid:\n",
      "\t\t\t-data: None\n",
      "\t\t\t-size: 0.1\n",
      "\t-specifications:\n",
      "\t\t-batch_size: 64\n",
      "\t\t-n_epoch: 200\n",
      "\t\t-pool_szs: [2, 2, 4]\n",
      "\t\t-top_k: 3\n",
      "\t\t-sliding_windows:\n",
      "\t\t\t-stride: 1\n",
      "\t\t\t-size: 32\n",
      "\t\t-autoencoder:\n",
      "\t\t\t-delta: 60\n",
      "\t\t\t-filters:\n",
      "\t\t\t\t-nfs: [64, 32, 16]\n",
      "\t\t\t\t-kss: [10, 5, 5]\n",
      "\t\t\t\t-output_size: 10\n",
      "After reading config\n",
      "-job_type: encoder_DCAE\n",
      "-alias: TiltABP\n",
      "-wandb:\n",
      "\t-use: False\n",
      "\t-entity: mi-santamaria\n",
      "\t-group: None\n",
      "\t-project: test-project\n",
      "-artifacts:\n",
      "\t-train: mi-santamaria/test-project/speed_6005:v0\n",
      "\t-valid:\n",
      "\t\t-data: None\n",
      "\t\t-size: 0.1\n",
      "-specifications:\n",
      "\t-batch_size: 64\n",
      "\t-n_epoch: 200\n",
      "\t-pool_szs: [2, 2, 4]\n",
      "\t-top_k: 3\n",
      "\t-sliding_windows:\n",
      "\t\t-stride: 1\n",
      "\t\t-size: 32\n",
      "\t-autoencoder:\n",
      "\t\t-delta: 60\n",
      "\t\t-filters:\n",
      "\t\t\t-nfs: [64, 32, 16]\n",
      "\t\t\t-kss: [10, 5, 5]\n",
      "\t\t\t-output_size: 10\n"
     ]
    }
   ],
   "source": [
    "import nbs_pipeline.utils.config as cfg\n",
    "config, job_type = cfg.get_artifact_config_DCAE(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: test-project\n"
     ]
    }
   ],
   "source": [
    "print(\"Project: \"+config.wandb_project)\n",
    "run = wandb.init(\n",
    "    entity          = config.wandb_entity,\n",
    "    project         = config.wandb_project,\n",
    "    group           = config.wandb_group,\n",
    "    job_type        = job_type,\n",
    "    allow_val_change= True,\n",
    "    mode            = 'online' if config.use_wandb else 'disabled',\n",
    "    config          = config,\n",
    "    resume          = False\n",
    ")\n",
    "config = run.config  # Object for storing hyperparameters\n",
    "# Botch to use artifacts offline\n",
    "artifacts_gettr = run.use_artifact if config.use_wandb else wandb_api.artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets\n",
    "\n",
    "To load the dataset we will download a specific dataset artifact from the collection of artifacts\n",
    "stored in the weights and biases (wandb) project associated to this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding window features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a continuous multivariate time-series data $D$ of dimension $d$ with $n$ time-steps, $D = X_1,X_2,\\dots,X_n$ , where each $X_i = \\{x_i^1,\\dots,x_i^d\\}$ . Let $w$ be the window width, $s$ the stride, and $t$ the start time of a sliding window in the data.\n",
    "\n",
    "Define a new matrix $Z_k$ where each row is a vector of size $w$ of data extracted from the $k^{th}$ dimension.\n",
    "\n",
    "\\begin{aligned}&Z_k(w,s,t)\\\\&\\quad =\\begin{bmatrix} x_{t}^k&\\quad x_{t+1}^k&\\quad \\dots&\\quad x_{t+w-1}^k \\\\ x_{t+s}^k&\\quad x_{t+s+1}^k&\\quad \\dots&\\quad x_{t+s+w-1}^k \\\\ \\vdots&\\quad \\vdots&\\quad \\ddots&\\quad \\vdots \\\\ x_{t+(r-1)s}^k&\\quad x_{t+(r-1)s+1}^k&\\quad \\dots&\\quad x_{t+(r-1)s+w-1}^k \\end{bmatrix} \\end{aligned}\n",
    "\n",
    "where $r$ is the number of desired rows, and $t+(r-1)s+w-1 \\le n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z$ is a $w \\times s \\times t$ matrix. The first step consists in slicing the original multivariate time series into slices of shape ($w \\times d$), as shown in this figure from the paper.\n",
    "<img src=\"https://i.imgur.com/R9Fx8uO.png\" style=\"width:800px;height:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters of this sliding window approach are given values by default here. If the value has been already set previously, that means this notebook is being called from a wandb sweep, and we must use the value that the sweep is bringing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = SlidingWindow(window_len=config.w, stride=config.stride, get_y=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2500, 2), (2469, 2, 32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_artifact = artifacts_gettr(config.train_artifact)\n",
    "df_train = train_artifact.to_df()\n",
    "X_train, _ = sw(df_train)\n",
    "df_train.shape, X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.valid_artifact:\n",
    "    valid_artifact = artifacts_gettr(config.valid_artifact)\n",
    "    df_val = valid_artifact.to_df()\n",
    "    X_valid, _ = sw(df_val)\n",
    "    df_val.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract important features from the multivariate time series data through Deep Convolutional Autoencoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Convolutional Auto Encoders (DCAE) is a powerful method for learning high-level and mid-level abstractions from low-level raw data. It has the ability to extract features from complex and large time-series in an unsupervised manner. This is useful to overcome the complexity of multivariate time-series.\n",
    "\n",
    "Compared to the conventional auto-encoder, DCAE has fewer parameters than the conventional auto-encoder which means less training time. Also, DCAE uses local information to reconstruct the signal while conventional auto-encoders utilize fully connected layers to globally do the reconstruction. DCAE is an unsupervised model for representation learning which maps inputs into a new representation space. It has two main parts which are the encoding part that is used to project the data into a set of feature spaces and the decoding part that reconstructs the original data. The latent space representation is the space where the data lie in the bottleneck layers.\n",
    "\n",
    "The loss function of the DCAE is defined as the error between the input and the output. DCAE aims to find a code for each input by minimizing the mean squared error (MSE) between its input (original data) and output (reconstructed data). The MSE is used which assists to minimize the loss; thus, the network is forced to learn a low-dimensional representation of the input.\n",
    "\n",
    "We will implement the DCAE of the paper [TimeCluster](https://link.springer.com/article/10.1007/s00371-019-01673-y), whose architecture is shown in the table below:\n",
    "\n",
    "![](https://i.imgur.com/3EjuAfQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the paper, the input shape is $60 \\times 3$, due to multivariate time series has 3 variables and the window size is 60. Generally, the size of the input/output of the autoencoder will depend on the shape of each slice obtained in the previos step. The number of latent features to be discovered is $60$ in the table above, but we can consider this as a free hyperparameter $\\delta$. Also, according to the paper: \"*The number of feature maps, size of filter and depth of the model are set based on the reconstruction error on validation set.*\". Thus, we must provide flexibility in the creation of the DCAE in terms of these hyperparameters.ยบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you are not using a config file, you can also uncomment the following cell and define the hyperparameters in the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_equal([len(x) for x in [config.nfs, config.kss, config.pool_szs]], \n",
    "          np.repeat(len(config.nfs), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model\n",
    "\n",
    "The implementation of the DCAE is done using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCAE_torch(\n",
      "  (downsample): Sequential(\n",
      "    (0): SameConv1d(\n",
      "      (conv1d_same): Conv1d(2, 64, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(32, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (5): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (latent_in): Linear(in_features=32, out_features=60, bias=True)\n",
      "    (latent_out): Linear(in_features=60, out_features=32, bias=True)\n",
      "    (reshape): Reshape(bs, 16, 2)\n",
      "  )\n",
      "  (upsample): Sequential(\n",
      "    (0): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): Upsample(scale_factor=4.0, mode=nearest)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (4): SameConv1d(\n",
      "      (conv1d_same): Conv1d(32, 64, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (6): SameConv1d(\n",
      "      (conv1d_same): Conv1d(64, 2, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 32]), torch.Size([1, 2, 32]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DCAE_torch(\n",
    "    c_in    = X_train.shape[1], \n",
    "    seq_len = config.w, \n",
    "    delta   = config.delta, \n",
    "    pool_szs= config.pool_szs, \n",
    "    nfs=config.nfs\n",
    ")\n",
    "print(m)\n",
    "foo = torch.rand(1, X_train.shape[1], config.w)\n",
    "foo.shape, m(foo).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with fastai Learner class, to abstract from Pytorch's training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x50 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "((#2223) [1239,1031,1192,783,1607,1193,2462,2403,876,1784...],\n",
       " (#246) [807,470,541,1497,1498,218,1151,560,227,1632...])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if config.valid_artifact:\n",
    "    X, y, splits  = combine_split_data(xs=[X_train, X_valid], ys=[X_train, X_valid])\n",
    "else:\n",
    "    X = X_train\n",
    "    y = X_train\n",
    "    splits = get_splits(np.arange(len(X)), valid_size=config.valid_size)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2015-09-11 20:46:00'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb Celda 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m tfms \u001b[39m=\u001b[39m [ToFloat(), ToFloat()]\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m batch_tfms \u001b[39m=\u001b[39m [TSStandardize(by_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)]\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m dls \u001b[39m=\u001b[39m get_ts_dls(X, y, splits\u001b[39m=\u001b[39;49msplits, tfms\u001b[39m=\u001b[39;49mtfms, batch_tfms\u001b[39m=\u001b[39;49mbatch_tfms)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dls\u001b[39m.\u001b[39mdataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshow(), ToTSTensor()(dls\u001b[39m.\u001b[39mdataset[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/lib/tsai/tsai/data/core.py:1051\u001b[0m, in \u001b[0;36mget_ts_dls\u001b[0;34m(X, y, splits, sel_vars, sel_steps, tfms, inplace, path, bs, batch_tfms, num_workers, device, shuffle_train, drop_last, weights, partial_n, sampler, sort, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m splits \u001b[39m=\u001b[39m _check_splits(X, splits)\n\u001b[1;32m   1050\u001b[0m create_dir(path, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1051\u001b[0m dsets \u001b[39m=\u001b[39m TSDatasets(X, y, splits\u001b[39m=\u001b[39;49msplits, sel_vars\u001b[39m=\u001b[39;49msel_vars, sel_steps\u001b[39m=\u001b[39;49msel_steps, tfms\u001b[39m=\u001b[39;49mtfms, inplace\u001b[39m=\u001b[39;49minplace)\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1053\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(X) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(weights), \u001b[39m'\u001b[39m\u001b[39mlen(X) != len(weights)\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/lib/tsai/tsai/data/core.py:466\u001b[0m, in \u001b[0;36mTSDatasets.__init__\u001b[0;34m(self, X, y, items, sel_vars, sel_steps, tfms, tls, n_inp, dl_type, inplace, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms \u001b[39m=\u001b[39m _remove_brackets(tfms)\n\u001b[1;32m    465\u001b[0m     lts \u001b[39m=\u001b[39m [NoTfmLists \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m TSTfmdLists \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mvectorized\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39melse\u001b[39;00m TfmdLists \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms]\n\u001b[0;32m--> 466\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls \u001b[39m=\u001b[39m L(lt(item, t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;49;00m lt,item,t \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(lts, items, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtfms))\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtyps \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(tl\u001b[39m.\u001b[39mitems[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tl\u001b[39m.\u001b[39mitems[\u001b[39m0\u001b[39m], torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtyps[i] \u001b[39mfor\u001b[39;00m i,tl \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls)]\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/foundation.py:98\u001b[0m, in \u001b[0;36m_L_Meta.__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(x,\u001b[39mcls\u001b[39m): \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/foundation.py:106\u001b[0m, in \u001b[0;36mL.__init__\u001b[0;34m(self, items, use_list, match, *rest)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, items\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39mrest, use_list\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, match\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m (use_list \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m is_array(items):\n\u001b[0;32m--> 106\u001b[0m         items \u001b[39m=\u001b[39m listify(items, \u001b[39m*\u001b[39;49mrest, use_list\u001b[39m=\u001b[39;49muse_list, match\u001b[39m=\u001b[39;49mmatch)\n\u001b[1;32m    107\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(items)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/basics.py:66\u001b[0m, in \u001b[0;36mlistify\u001b[0;34m(o, use_list, match, *rest)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mlist\u001b[39m): res \u001b[39m=\u001b[39m o\n\u001b[1;32m     65\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_array(o): res \u001b[39m=\u001b[39m [o]\n\u001b[0;32m---> 66\u001b[0m \u001b[39melif\u001b[39;00m is_iter(o): res \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(o)\n\u001b[1;32m     67\u001b[0m \u001b[39melse\u001b[39;00m: res \u001b[39m=\u001b[39m [o]\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m match \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/lib/tsai/tsai/data/core.py:466\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms \u001b[39m=\u001b[39m _remove_brackets(tfms)\n\u001b[1;32m    465\u001b[0m     lts \u001b[39m=\u001b[39m [NoTfmLists \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m TSTfmdLists \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(t, \u001b[39m'\u001b[39m\u001b[39mvectorized\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39melse\u001b[39;00m TfmdLists \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms]\n\u001b[0;32m--> 466\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls \u001b[39m=\u001b[39m L(lt(item, t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m lt,item,t \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(lts, items, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms))\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtyps \u001b[39m=\u001b[39m [\u001b[39mtype\u001b[39m(tl\u001b[39m.\u001b[39mitems[\u001b[39m0\u001b[39m]) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(tl\u001b[39m.\u001b[39mitems[\u001b[39m0\u001b[39m], torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtyps[i] \u001b[39mfor\u001b[39;00m i,tl \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtls)]\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/foundation.py:98\u001b[0m, in \u001b[0;36m_L_Meta.__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mcls\u001b[39m, x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(x,\u001b[39mcls\u001b[39m): \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/data/core.py:365\u001b[0m, in \u001b[0;36mTfmdLists.__init__\u001b[0;34m(self, items, tfms, use_list, do_setup, split_idx, train_setup, splits, types, verbose, dl_type)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mif\u001b[39;00m do_setup:\n\u001b[1;32m    364\u001b[0m     pv(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSetting up \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, verbose)\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(train_setup\u001b[39m=\u001b[39;49mtrain_setup)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/data/core.py:392\u001b[0m, in \u001b[0;36mTfmdLists.setup\u001b[0;34m(self, train_setup)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtfms\u001b[39m.\u001b[39mfs:\n\u001b[1;32m    391\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mappend(\u001b[39mgetattr\u001b[39m(f, \u001b[39m'\u001b[39m\u001b[39minput_types\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m(x)))\n\u001b[0;32m--> 392\u001b[0m         x \u001b[39m=\u001b[39m f(x)\n\u001b[1;32m    393\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mappend(\u001b[39mtype\u001b[39m(x))\n\u001b[1;32m    394\u001b[0m types \u001b[39m=\u001b[39m L(t \u001b[39mif\u001b[39;00m is_listy(t) \u001b[39melse\u001b[39;00m [t] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtypes)\u001b[39m.\u001b[39mconcat()\u001b[39m.\u001b[39munique()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m'\u001b[39;49m\u001b[39mencodes\u001b[39;49m\u001b[39m'\u001b[39;49m, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, fn, x, split_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m split_idx\u001b[39m!=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, fn), x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(f, x_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/lib/tsai/tsai/data/core.py:151\u001b[0m, in \u001b[0;36mToFloat.encodes\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencodes\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(o, dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2015-09-11 20:46:00'"
     ]
    }
   ],
   "source": [
    "tfms = [ToFloat(), ToFloat()]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms)\n",
    "dls.dataset[0][0].show(), ToTSTensor()(dls.dataset[0][1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = DCAE_torch(\n",
    "    c_in=X_train.shape[1], \n",
    "    seq_len=config.w, \n",
    "    delta=config.delta, \n",
    "    pool_szs=config.pool_szs, \n",
    "    nfs=config.nfs\n",
    ")\n",
    "learn = Learner(\n",
    "    dls=dls, \n",
    "    model=m, \n",
    "    loss_func=nn.MSELoss(),\n",
    "    opt_func=Adam, \n",
    "    cbs=[WandbCallback(log_preds=False)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCAE_torch(\n",
      "  (downsample): Sequential(\n",
      "    (0): SameConv1d(\n",
      "      (conv1d_same): Conv1d(2, 64, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "    (1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(32, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (5): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (latent_in): Linear(in_features=32, out_features=60, bias=True)\n",
      "    (latent_out): Linear(in_features=60, out_features=32, bias=True)\n",
      "    (reshape): Reshape(bs, 16, 2)\n",
      "  )\n",
      "  (upsample): Sequential(\n",
      "    (0): Conv1d(16, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): Upsample(scale_factor=4.0, mode=nearest)\n",
      "    (2): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (4): SameConv1d(\n",
      "      (conv1d_same): Conv1d(32, 64, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "    (5): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (6): SameConv1d(\n",
      "      (conv1d_same): Conv1d(64, 2, kernel_size=(10,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='166' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      83.00% [166/200 17:50&lt;03:39]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>75044072.000000</td>\n",
       "      <td>72809888.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>62128828.000000</td>\n",
       "      <td>60443380.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50522812.000000</td>\n",
       "      <td>49005156.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>47698784.000000</td>\n",
       "      <td>46399612.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45330068.000000</td>\n",
       "      <td>44454116.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>44892032.000000</td>\n",
       "      <td>43797280.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>43626852.000000</td>\n",
       "      <td>43365208.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>43599628.000000</td>\n",
       "      <td>43486324.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>43911948.000000</td>\n",
       "      <td>43052980.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>41969504.000000</td>\n",
       "      <td>41785784.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>41285872.000000</td>\n",
       "      <td>39996200.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>38250828.000000</td>\n",
       "      <td>36128880.000000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>33414382.000000</td>\n",
       "      <td>31940764.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>32129456.000000</td>\n",
       "      <td>30389268.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>32097528.000000</td>\n",
       "      <td>29272808.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>29732402.000000</td>\n",
       "      <td>29463198.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>28964554.000000</td>\n",
       "      <td>35782108.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>28810482.000000</td>\n",
       "      <td>27393940.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>28683484.000000</td>\n",
       "      <td>28674348.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>28491284.000000</td>\n",
       "      <td>27158330.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>27245758.000000</td>\n",
       "      <td>25392546.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>26781280.000000</td>\n",
       "      <td>25706610.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>26685564.000000</td>\n",
       "      <td>27143796.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>25712592.000000</td>\n",
       "      <td>25095566.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>25269650.000000</td>\n",
       "      <td>26022266.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>24578454.000000</td>\n",
       "      <td>22792348.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>24163856.000000</td>\n",
       "      <td>23354918.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>24287014.000000</td>\n",
       "      <td>22079688.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>24380650.000000</td>\n",
       "      <td>21779304.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>24219410.000000</td>\n",
       "      <td>23156342.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>23705154.000000</td>\n",
       "      <td>20726494.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>22476016.000000</td>\n",
       "      <td>21476466.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>22358874.000000</td>\n",
       "      <td>20988638.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>21742776.000000</td>\n",
       "      <td>23177548.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>22479520.000000</td>\n",
       "      <td>21356686.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>22021562.000000</td>\n",
       "      <td>20842642.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>21990660.000000</td>\n",
       "      <td>20468726.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>21856436.000000</td>\n",
       "      <td>21312832.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>21120076.000000</td>\n",
       "      <td>21349890.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>21522480.000000</td>\n",
       "      <td>21253328.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>21441780.000000</td>\n",
       "      <td>20609890.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>20976184.000000</td>\n",
       "      <td>20186970.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>21696112.000000</td>\n",
       "      <td>21293038.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>20903314.000000</td>\n",
       "      <td>21437104.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>21507186.000000</td>\n",
       "      <td>19343732.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>22149334.000000</td>\n",
       "      <td>29756512.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>20876948.000000</td>\n",
       "      <td>18373440.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>19825920.000000</td>\n",
       "      <td>22148700.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>20398564.000000</td>\n",
       "      <td>19283612.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>21006836.000000</td>\n",
       "      <td>19183226.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>19545398.000000</td>\n",
       "      <td>18088408.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>19453290.000000</td>\n",
       "      <td>17962850.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>19535620.000000</td>\n",
       "      <td>18185038.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>19847682.000000</td>\n",
       "      <td>19716426.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>18846394.000000</td>\n",
       "      <td>18532968.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>19099382.000000</td>\n",
       "      <td>19130270.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>18429992.000000</td>\n",
       "      <td>18914936.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>18919824.000000</td>\n",
       "      <td>16618024.000000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>19145538.000000</td>\n",
       "      <td>16802646.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>18358502.000000</td>\n",
       "      <td>16925276.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>18308312.000000</td>\n",
       "      <td>16287546.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>17595984.000000</td>\n",
       "      <td>19434930.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>17319040.000000</td>\n",
       "      <td>16161036.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>16401969.000000</td>\n",
       "      <td>17081908.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>16272578.000000</td>\n",
       "      <td>17457718.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>16165794.000000</td>\n",
       "      <td>15318105.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>15837191.000000</td>\n",
       "      <td>18204004.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>15226187.000000</td>\n",
       "      <td>14847867.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>14483605.000000</td>\n",
       "      <td>13140015.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>15379132.000000</td>\n",
       "      <td>14662159.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>14054905.000000</td>\n",
       "      <td>12886138.000000</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>14146913.000000</td>\n",
       "      <td>11715665.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>13695482.000000</td>\n",
       "      <td>11616572.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>13362477.000000</td>\n",
       "      <td>11524277.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>12793366.000000</td>\n",
       "      <td>12703123.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>11503652.000000</td>\n",
       "      <td>10713831.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>12915009.000000</td>\n",
       "      <td>12100668.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>12446353.000000</td>\n",
       "      <td>13932048.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>11586708.000000</td>\n",
       "      <td>10614971.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>11707295.000000</td>\n",
       "      <td>13165094.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>11814949.000000</td>\n",
       "      <td>26899236.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>10930660.000000</td>\n",
       "      <td>9930923.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>11308319.000000</td>\n",
       "      <td>10948003.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>11019005.000000</td>\n",
       "      <td>9416843.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>11102173.000000</td>\n",
       "      <td>9098885.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>10123683.000000</td>\n",
       "      <td>13758571.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>10273821.000000</td>\n",
       "      <td>13232308.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>9317779.000000</td>\n",
       "      <td>9099483.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>9580511.000000</td>\n",
       "      <td>10145277.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>9596376.000000</td>\n",
       "      <td>11364555.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>11226873.000000</td>\n",
       "      <td>10347591.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>8960246.000000</td>\n",
       "      <td>11094173.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>9500314.000000</td>\n",
       "      <td>10706379.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>8814420.000000</td>\n",
       "      <td>8917585.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>8800023.000000</td>\n",
       "      <td>12095342.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>9226716.000000</td>\n",
       "      <td>8813745.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>8049739.500000</td>\n",
       "      <td>8182619.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>7257929.500000</td>\n",
       "      <td>6767225.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>9667126.000000</td>\n",
       "      <td>8695808.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>8212072.500000</td>\n",
       "      <td>8706272.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8426574.000000</td>\n",
       "      <td>8286326.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>7514972.000000</td>\n",
       "      <td>7553794.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>8118894.500000</td>\n",
       "      <td>8291002.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>7002810.500000</td>\n",
       "      <td>7180419.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>8049301.000000</td>\n",
       "      <td>7136604.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>7933552.500000</td>\n",
       "      <td>7935056.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>7684760.000000</td>\n",
       "      <td>6853072.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>8120077.500000</td>\n",
       "      <td>7978878.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>8459132.000000</td>\n",
       "      <td>6537900.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>8260429.500000</td>\n",
       "      <td>9242618.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>7345881.500000</td>\n",
       "      <td>8023361.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>7443323.500000</td>\n",
       "      <td>8175314.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>6634994.000000</td>\n",
       "      <td>5971853.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>5911878.500000</td>\n",
       "      <td>6512048.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>5880693.500000</td>\n",
       "      <td>7985309.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>6421095.500000</td>\n",
       "      <td>6131793.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>6407282.000000</td>\n",
       "      <td>7286557.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>6462199.000000</td>\n",
       "      <td>6023754.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>6404026.500000</td>\n",
       "      <td>6129185.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>5969960.500000</td>\n",
       "      <td>6987642.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6402039.500000</td>\n",
       "      <td>5658050.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>5593379.500000</td>\n",
       "      <td>8858587.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>5782745.000000</td>\n",
       "      <td>6849567.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>6103340.500000</td>\n",
       "      <td>5379609.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>5164353.000000</td>\n",
       "      <td>5541562.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>6020822.500000</td>\n",
       "      <td>6036347.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>6652758.000000</td>\n",
       "      <td>5826862.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>4975128.000000</td>\n",
       "      <td>5496855.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>5843964.000000</td>\n",
       "      <td>5867849.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>5068938.500000</td>\n",
       "      <td>6417830.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5473530.500000</td>\n",
       "      <td>5709859.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>5072374.000000</td>\n",
       "      <td>4885323.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>4641986.000000</td>\n",
       "      <td>4665614.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>4821530.500000</td>\n",
       "      <td>4130806.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>4453663.500000</td>\n",
       "      <td>4537731.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>4482975.500000</td>\n",
       "      <td>5401421.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>4954863.000000</td>\n",
       "      <td>4877367.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>4377149.000000</td>\n",
       "      <td>4509240.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>4271983.500000</td>\n",
       "      <td>4761744.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>4661966.500000</td>\n",
       "      <td>6468694.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3842879.750000</td>\n",
       "      <td>3953627.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>4173885.000000</td>\n",
       "      <td>4239227.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3850804.750000</td>\n",
       "      <td>3856209.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>4314743.000000</td>\n",
       "      <td>4931784.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>3973102.750000</td>\n",
       "      <td>3823188.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3571857.500000</td>\n",
       "      <td>3813881.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>3803047.000000</td>\n",
       "      <td>3934585.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>3981965.250000</td>\n",
       "      <td>3638639.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>3330394.500000</td>\n",
       "      <td>3300237.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>3462657.000000</td>\n",
       "      <td>4416778.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3653739.250000</td>\n",
       "      <td>3480552.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>3301484.000000</td>\n",
       "      <td>3395385.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3318019.250000</td>\n",
       "      <td>3220189.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>3152299.750000</td>\n",
       "      <td>3716511.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>3990235.750000</td>\n",
       "      <td>3843237.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>3223004.000000</td>\n",
       "      <td>3933947.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>3077987.750000</td>\n",
       "      <td>3659756.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>3062955.500000</td>\n",
       "      <td>3114461.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>3067666.000000</td>\n",
       "      <td>3136184.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>3023977.250000</td>\n",
       "      <td>3273223.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2924880.250000</td>\n",
       "      <td>3430243.750000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2722416.500000</td>\n",
       "      <td>2928224.250000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2754687.750000</td>\n",
       "      <td>3434264.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2643544.750000</td>\n",
       "      <td>2730385.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2864807.500000</td>\n",
       "      <td>3013850.500000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2575533.500000</td>\n",
       "      <td>2681278.000000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='483' class='' max='562' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      85.94% [483/562 00:05&lt;00:00 2551074.0000]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Enabling Vs Code execution ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb Celda 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m learn \u001b[39m=\u001b[39m Learner(dls\u001b[39m=\u001b[39mdls, model\u001b[39m=\u001b[39mm, loss_func\u001b[39m=\u001b[39mnn\u001b[39m.\u001b[39mMSELoss(), opt_func\u001b[39m=\u001b[39mAdam, \n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                 cbs\u001b[39m=\u001b[39m[WandbCallback(log_preds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)])\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m lr_valley, lr_steep \u001b[39m=\u001b[39m learn\u001b[39m.\u001b[39mlr_find(suggest_funcs\u001b[39m=\u001b[39m[valley, steep])\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m learn\u001b[39m.\u001b[39;49mfit_one_cycle(config\u001b[39m.\u001b[39;49mepochs, lr_max\u001b[39m=\u001b[39;49mlr_valley)\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f64766174732d6a7570797465722d31222c2273657474696e6773223a7b22686f7374223a227373683a2f2f67342e6574736973692e75706d2e6573227d7d/home/macu/work/nbs_pipeline/02b_encoder_DCAE.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m learn\u001b[39m.\u001b[39mplot_metrics()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/callback/schedule.py:119\u001b[0m, in \u001b[0;36mfit_one_cycle\u001b[0;34m(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    116\u001b[0m lr_max \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([h[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mhypers])\n\u001b[1;32m    117\u001b[0m scheds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, lr_max\u001b[39m/\u001b[39mdiv, lr_max, lr_max\u001b[39m/\u001b[39mdiv_final),\n\u001b[1;32m    118\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mmom\u001b[39m\u001b[39m'\u001b[39m: combined_cos(pct_start, \u001b[39m*\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoms \u001b[39mif\u001b[39;00m moms \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m moms))}\n\u001b[0;32m--> 119\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(n_epoch, cbs\u001b[39m=\u001b[39;49mParamScheduler(scheds)\u001b[39m+\u001b[39;49mL(cbs), reset_opt\u001b[39m=\u001b[39;49mreset_opt, wd\u001b[39m=\u001b[39;49mwd, start_epoch\u001b[39m=\u001b[39;49mstart_epoch)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:256\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt\u001b[39m.\u001b[39mset_hypers(lr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr \u001b[39mif\u001b[39;00m lr \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m lr)\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch \u001b[39m=\u001b[39m n_epoch\n\u001b[0;32m--> 256\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_fit, \u001b[39m'\u001b[39;49m\u001b[39mfit\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelFitException, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_end_cleanup)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:245\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch):\n\u001b[1;32m    244\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch\u001b[39m=\u001b[39mepoch\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch, \u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelEpochException)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:239\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_epoch_train()\n\u001b[1;32m    240\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:231\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_epoch_train\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    230\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtrain\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_batches, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelTrainException)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mall_batches\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdl): \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_batch(\u001b[39m*\u001b[39;49mo)\n",
      "File \u001b[0;32m~/lib/tsai/tsai/learner.py:40\u001b[0m, in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m     38\u001b[0m b_on_device \u001b[39m=\u001b[39m to_device(b, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m b\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split(b_on_device)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_with_events(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_one_batch, \u001b[39m'\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m'\u001b[39;49m, CancelBatchException)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:193\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_with_events\u001b[39m(\u001b[39mself\u001b[39m, f, event_type, ex, final\u001b[39m=\u001b[39mnoop):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mtry\u001b[39;00m: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbefore_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  f()\n\u001b[1;32m    194\u001b[0m     \u001b[39mexcept\u001b[39;00m ex: \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_cancel_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mafter_\u001b[39m\u001b[39m{\u001b[39;00mevent_type\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/fastai/learner.py:205\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_one_batch\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxb)\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mafter_pred\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    207\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39myb):\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/dvats/encoder.py:53\u001b[0m, in \u001b[0;36mDCAE_torch.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 53\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x)\n\u001b[1;32m     54\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbottleneck(x)\n\u001b[1;32m     55\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(x)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lib/tsai/tsai/models/layers.py:166\u001b[0m, in \u001b[0;36mSameConv1d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding \u001b[39m=\u001b[39m same_padding1d(x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mks, dilation\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation) \u001b[39m#stride=self.stride not used in padding calculation!\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1d_same(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding)(x))\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/nn/modules/conv.py:309\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    307\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                     _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 309\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv1d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    310\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr_valley, lr_steep = learn.lr_find(suggest_funcs=[valley, steep])\n",
    "learn.fit_one_cycle(config.epochs, lr_max=lr_valley)\n",
    "learn.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "To track the performance of this model fit, go to the project dashboard in Weights & Biases. The link is provided at the beginning of this notebook, after the execution of the function `wandb.init()'' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, log the learner to be used by the next notebook in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_learn = learn.export_and_get()\n",
    "if config.use_wandb: \n",
    "    ar = ReferenceArtifact(aux_learn, f'dcae', type='learner', metadata=dict(run.config))\n",
    "    run.log_artifact(ar, aliases=f'run-{run.project}-{run.id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline models (To rewrite)\n",
    "\n",
    "Calculate baseline models taking into account that the best prediction is the average and median value of each of the windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the autoencoder\n",
    "\n",
    "Let's validate the autoencoder quality visually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the best and the worst k predictions using the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = Interpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_losses = interp.top_losses(3)\n",
    "top_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in top_losses.indices: dls.dataset[i][0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d45d555be0220b07bf61be557bfa0ebbf7a95015976aec9a23277863e1bd4593"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
