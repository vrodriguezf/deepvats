include: !include "base.yaml"
#########################################################
########### NBS - PIPELINE CONFIGURATION FILE ###########
########### 02a - ENCODER DCAE                ###########
#########################################################
###### Author: Maria Inmaculada Santamaria-Valenzuela   #
###### Date: 08-2023                                    #
#########################################################

######################
# 02a - ENCODER DCAE #
######################

configuration:
  job_type: 'encoder_DCAE'
  alias: *alias
  wandb:
    use: *use_wandb
    entity: *wdb_user
    group: null
    project: *wdb_project
  artifacts:
    # complete training artifact path
    train_prefix: enc_prefix: !join [ *wdb_user, '/', *wdb_project, '/', *alg]
    #!join [ *wdb_user, '/', *wdb_project, '/', *fname, ':v', *wdb_version]
    # complete name (None for random validation set)
    valid:
      data: null
      # In case valid_artifact is None, this will set the percentage of random items to go to val
      size: 0.1
  specifications:
    batch_size: 64
    # Number of epochs to train for
    n_epoch: 200
    pool_szs: [2,2,4]
    # Number of elements to analyse for the top losses
    top_k: 3
    sliding_windows:
      # n datapoints the window is moved ahead along the sequence in the sliding window
      stride: 1
      # window size for the sliding window (taxi=48, steamflow=640)
      size: 32 #w
    autoencoder:
      # Size of the autoencoder bottleneck layer
      delta: 60
      filters:
        #The number of filters in the last conv layer must be equal to the product of pool sizes
        nfs: [64,32,16]
        #TODO: confirmar
        #Kernel size determines the area each filter considers when processing input data.
        kss: [10,5,5]
        output_size: 10
    

