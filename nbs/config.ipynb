{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2ed463-8d9f-40cd-be11-d8d1d510fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34d5df84-5213-4795-bb13-2bb0b7f922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d5247-f6be-4126-8480-b6b307798212",
   "metadata": {},
   "source": [
    "# Configuration functions \n",
    "> This notebook introduces configuration functions designed to leverage proposed .yaml files, providing a more transparent and automated approach to artefact definition. While this method offers enhanced clarity, the option for direct definition within each notebook is also fully supported and remains practical.\n",
    ">\n",
    "> <span style=\"color: red;\"> TODO: Check for possible simplifications. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7184a-93ba-49c1-a769-0b53cad5e08e",
   "metadata": {},
   "source": [
    "First, we import\n",
    "- os and sys for basic access to files\n",
    "- yaml for .yaml reading\n",
    "- tsai.basics for using tsai artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6faaa9-abda-444c-ad5c-b1345ac9306b",
   "metadata": {},
   "source": [
    "## Basic configuration and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cc1c68-f710-46eb-9fc8-e57f7f020f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "from tsai.basics import *\n",
    "from dvats.utils import print_flush"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64aea40-28b4-44cb-ada2-d77927836eed",
   "metadata": {},
   "source": [
    "As the yml are saved in ../nbs_pipeline/config, we will need to add '..' path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af853d2e-758c-4691-9127-9239ab46544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "sys.path.append(os.path.abspath('../nbs_pipeline'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556b132-40f4-4daa-a50f-f38833a24aa5",
   "metadata": {},
   "source": [
    "An custom_error(message) function is added just for basic error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3a27344-8e4b-4738-badb-73c1cbe59199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def custom_error(message: str):\n",
    "    \"\"\"\n",
    "    This message raises an exception ensuring red-coloured error message is displayed\n",
    "    \"\"\"\n",
    "    # Change to red color ANSI code\n",
    "    red_start = \"\\033[91m\"\n",
    "    # Back to original color ANSI code\n",
    "    reset = \"\\033[0m\"\n",
    "    raise Exception(red_start + message + reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8af711-39dc-4d65-8142-04e8ec7fad8a",
   "metadata": {},
   "source": [
    "## Yaml reading auxiliar functions\n",
    "> Defining basics functions to read yml in order to build Attrdict's\n",
    "\n",
    "In the version of YAML we are utilising, the '!join' constructor is not present. Therefore, it becomes necessary to define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c06dfbe-9eb0-47e2-8b02-06f977fa5267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def join_constructor(loader: yaml.Loader, node: yaml.Node) -> str:\n",
    "    \"\"\"\n",
    "    This function adds the '!join' constructor to the YAML parsing process. \n",
    "    It constructs a sequence from the provided node and then joins the elements of the sequence into a single string.\n",
    "    \"\"\"\n",
    "    seq = loader.construct_sequence(node)\n",
    "    return ''.join(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7efaffe9-b8a0-47f1-9082-9480fdf166f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recursive_attrdict(d: dict) -> AttrDict:\n",
    "    \"\"\" Recursively converts a dictionary into an AttrDict, including all nested dictionaries. \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        return AttrDict({k: recursive_attrdict(v) for k, v in d.items()})\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e338cefb-6a7b-42b7-be07-aebc0a89ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def replace_includes_with_content(\n",
    "    filename: str, \n",
    "    path: str = \"./\", \n",
    "    verbose : int = 0\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    This function processes a YAML file.\n",
    "    It replaces '!include' directives with the content of the specified files. \n",
    "    It takes a filename and a path, reads the file, and iteratively replaces any '!include' directives with the content of the included files.\n",
    "    The final processed content, with all '!include' directives substituted is returned as a string.\n",
    "    \"\"\"\n",
    "    if (verbose > 0):\n",
    "        print_flush(\"... About to replace includes with content\")\n",
    "    with open(path+filename, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "        # Mientras exista una directiva !include en el contenido, sigue reemplazándola\n",
    "        while \"!include\" in content:\n",
    "            # Obtén la posición de la directiva\n",
    "            start_idx = content.find('!include')\n",
    "            # Encuentra el inicio y el final de las comillas que contienen el nombre del archivo\n",
    "            start_quote_idx = content.find('\"', start_idx) + 1\n",
    "            end_quote_idx = content.find('\"', start_quote_idx)\n",
    "            \n",
    "            # Extrae el nombre del archivo\n",
    "            include_filename = content[start_quote_idx:end_quote_idx]\n",
    "            \n",
    "            # Lee el archivo incluido\n",
    "            with open(path+include_filename, 'r', encoding='utf-8') as include_file:\n",
    "                included_content = include_file.read()\n",
    "            \n",
    "            # Reemplaza la directiva por el contenido del archivo incluido\n",
    "            content = content[:start_idx] + included_content + content[end_quote_idx+1:]\n",
    "        \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cbf35-6b6e-4da7-9fe3-42725c0eb40e",
   "metadata": {},
   "source": [
    "## Project specific yaml reading functions\n",
    "> Basic configuration is read from `../config/base.yml`\n",
    "> It contains the following information:\n",
    "> - User Preferences: `user_preferences`\n",
    ">   - General configuration: `user_preferences`\n",
    ">   \n",
    ">     | Key         | Description                                             | Example Value |\n",
    ">     |-------------|---------------------------------------------------------|---------------|\n",
    ">     | `use_wandb` | Indicates whether to use wandb for experiment tracking. | `true`        |\n",
    ">   \n",
    ">   - Wandb Configuration: `user_preferences.wdb`\n",
    ">     \n",
    ">     | Key                        | Description                                       | Example Value           |\n",
    ">     |----------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `user`                     | The user name for wandb.                          | `mi-santamaria`         |\n",
    ">     | `project_name`             | The project name in wandb.                        | `deepvats`              |\n",
    ">     | `version`                  | Specifies the version for wandb.                  | `'latest'`              |\n",
    ">     | `mode`                     | Sets the mode for wandb.                          | `'offline'`             |\n",
    ">     | `artifacts_path`           | The path for storing wandb artifacts.             | `'./data/wandb_artifacts'`|\n",
    ">   \n",
    ">   - Data configuration: `user_preferences.data`\n",
    ">     \n",
    ">     | Key                      | Description                                       | Example Value           |\n",
    ">     |--------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `folder`                 | The folder where the data is stored.              | `'~/data/'`             |\n",
    ">     | `fname`                  | The file name of the dataset used.                | `'solar_4_seconds_dataset'` |\n",
    ">     | `ftype`                  | The file type of the dataset.                     | `'.tsf'`                |\n",
    ">     | `cols`                   | The columns to be used from the dataset.          | `[]` (all columns)      |\n",
    ">     | `freq`                   | The frequency of the dataset.                     | `'4s'`                  |\n",
    ">   \n",
    ">   - Artifact Configuration: `user_preferences.artifact`\n",
    ">     \n",
    ">     | Key                      | Description                                       | Example Value           |\n",
    ">     |--------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `alias`                  | Alias for the resulting artifact of the run.      | `'solar_4_seconds'`     |\n",
    ">     | `algorithm`              | The algorithm used in the process.                | `'mvp'`                 |\n",
    ">   \n",
    ">   - Directory Configuration: `user_preferences.directories`\n",
    ">     \n",
    ">     | Key                      | Description                                       | Example Value           |\n",
    ">     |--------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `tmp`                    | Folder for storing temporary files.               | `'tmp'`                 |\n",
    ">     | `data`                   | Path for the data.                                | Combination of *path, *fname, *ftype |\n",
    ">\n",
    "> - Data Specifications: `data`\n",
    ">   - Data Details: `data`\n",
    ">     \n",
    ">     | Key                      | Description                                       | Example Value           |\n",
    ">     |--------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `name`                   | The name of the data file.                        | Refers to *fname        |\n",
    ">     | `path`                   | The path of the data file.                        | Refers to *data_path    |\n",
    ">     | `alias`                  | The alias of the data artifact.                   | Refers to *alias        |\n",
    ">     | `cols`                   | Columns of interest from the dataset.             | Refers to *cols         |\n",
    ">     | `csv_config`             | Configuration for CSV files, if applicable.       | `{}` (if not required)  |\n",
    ">     | `date_offset`            | Offset used for setting the date index.           | `null` (if not used)    |\n",
    ">     | `date_format`            | Default date format for .tsf files.               | `'%Y-%m-%d %H:%M:%S'`   |\n",
    ">     | `freq`                   | Frequency of the data.                            | Refers to *freq         |\n",
    ">     | `joining_train_test`     | Linking training and testing data.                | `false`                 |\n",
    ">     | `missing_values`         | Handling missing values.                          | `technique: null, constant: null` |\n",
    ">     | `normalize_training`     | Indicates whether to normalize training data.     | `false`                 |\n",
    ">     | `range_training`         | Specifies training ranges.                        | `null`                  |\n",
    ">     | `range_testing`          | Specifies testing ranges.                         | `null`                  |\n",
    ">     | `resampling_freq`        | Frequency for resampling the data.                | `null`                  |\n",
    ">     | `start_date`             | Starting date for the dataset.                    | `null`                  |\n",
    ">     | `test_split`             | Ratio of the test set.                            | `null`                  |\n",
    ">     | `time_col`               | Column containing the timestamp.                  | `null`                  |\n",
    ">   \n",
    "> - Wandb Specifications: `wandb`\n",
    ">     \n",
    ">     | Key                      | Description                                       | Example Value           |\n",
    ">     |--------------------------|---------------------------------------------------|-------------------------|\n",
    ">     | `user`                   | User name in wandb specifications.                | Refers to *wdb_user     |\n",
    ">     | `dir`                    | Directory for wandb.                              | Combination of '~/', *wdb_project |\n",
    ">     | `enabled`                | Indicates if wandb is enabled.                    | `False`                 |\n",
    ">     | `group`                  | Group for the wandb runs.                         | `null`                  |\n",
    ">     | `log_learner`            | Whether to log the learner to wandb.              | `False`                 |\n",
    ">     | `mode`                   | Mode for wandb operation.                         | Refers to *wdb_mode     |\n",
    ">     | `project`                | The wandb project name.                           | Refers to *wdb_project  |\n",
    ">     | `version`                | Version for the wandb.                            | Refers to *wdb_version  |\n",
    ">     | `artifacts_path`         | Path for storing the TSArtifact in wandb.         | Refers to *artifacts_path |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36c82c69-859c-40a0-9cc2-3e4144c362b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_config(\n",
    "        verbose: int = 0,\n",
    "        filename: str = \"base\"\n",
    ") -> AttrDict:\n",
    "    \"\"\"\n",
    "    This function \n",
    "    - Reads the content in '../config/base.yml' and \n",
    "    - Returns its content as AttrDict.\n",
    "    verbose option can be changed to True for displaying debugging messages. \n",
    "    \"\"\"\n",
    "    # Build file path\n",
    "    filename = filename+\".yaml\"\n",
    "    path = \"./config/\"\n",
    "    # Debug messages\n",
    "    if (verbose > 0):\n",
    "        current_directory = os.getcwd()\n",
    "        print_flush(\"Current: \" + current_directory)\n",
    "        print_flush(\"yml: \"+ path + filename)\n",
    "    \n",
    "    # Add join constructor\n",
    "    yaml.add_constructor('!join', join_constructor)\n",
    "    \n",
    "    if (verbose > 0): \n",
    "        print_flush(\"Getting content\"+ path + filename)\n",
    "    \n",
    "    # Get file content\n",
    "    full_content = replace_includes_with_content(filename, path, verbose)\n",
    "    \n",
    "    if (verbose > 0):\n",
    "        print_flush(\"Load content\"+ path + filename)\n",
    "    \n",
    "    # Load content \n",
    "    config = yaml.load(full_content, Loader=yaml.FullLoader)\n",
    "    \n",
    "    # Return content it as AttrDict\n",
    "    return recursive_attrdict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee136ea-0fe1-4b4d-a1ef-e8b9414a1766",
   "metadata": {},
   "source": [
    "### Build the encoder artifact from already read functions\n",
    "> The encoder artifact is needed for training the model.\n",
    "> \n",
    "> To automatize its selection, its W&B name is built up from its latest version\n",
    "> \n",
    "> <span style=\"color: red;\"> TODO: Change for selecting specific version. Version should be selected in .yml file too </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b7dc69-c76c-4056-b2bb-bf732e1e0530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def build_enc_artifact(\n",
    "    config  : AttrDict, \n",
    "    verbose : int  = 0,\n",
    "    swv     : bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Build enc_artifact name from config data.\n",
    "    \"\"\"\n",
    "    version = config.user_preferences.wdb.version\n",
    "    enc_artifact = config.configuration.encoder.artifacts.train.enc_prefix\n",
    "\n",
    "    if swv:\n",
    "        if not enc_artifact.endswith(\"-SWV\"):\n",
    "            enc_artifact += \"-SWV\"\n",
    "    else:\n",
    "        if enc_artifact.endswith(\"-SWV\"):\n",
    "            enc_artifact = enc_artifact[:-4]  # Remover '-SWV'\n",
    "    \n",
    "    if (version == 'latest'):\n",
    "        enc_artifact+=\":latest\"\n",
    "    else:\n",
    "        enc_artifact=enc_artifact+\":v\"+version\n",
    "    if (verbose > 0):\n",
    "        print_flush(\"enc_artifact: \"+enc_artifact)\n",
    "    return enc_artifact\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f96e8-9a6a-464c-ab66-6137bb9a85c3",
   "metadata": {},
   "source": [
    "### Build project data\n",
    "> Read user, project, version and dataset artifact name form already got information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9435b302-cc11-4e75-8d9b-d746456b1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_project_data(verbose : int = 0) -> [str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Retrieves project data including user, project name, version, and data name. \n",
    "    It accesses configuration settings, processes them, and optionally prints the project configuration. \n",
    "    Returns a tuple containing user, project, version, and data information.\n",
    "    \"\"\"\n",
    "    config      = get_config()\n",
    "    project     = config.wandb.project\n",
    "    user        = config.wandb.user\n",
    "    version     = config.wandb.version\n",
    "    data_name   = config.data.alias\n",
    "    if (version != \"latest\"):\n",
    "        version = 'v' +version\n",
    "    data        = data_name +\":\"+version\n",
    "    if verbose > 0:\n",
    "        dashes = '-----------'        \n",
    "        print_flush(dashes+\"Project configuration\"+dashes)\n",
    "        print_flush(\"user: \" + user)\n",
    "        print_flush(\"project: \" + project)\n",
    "        print_flush(\"version: \" + version)\n",
    "        print_flush(\"data: \"+ data)\n",
    "        print_flush(dashes+\"Project configuration\"+dashes)\n",
    "    return user, project, version, data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065856d6-a217-490c-821c-dd4abf205344",
   "metadata": {},
   "source": [
    "### Get training artifact name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd10beae-87e1-4738-bd01-4c1162f3bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_train_artifact(user: str, project: str, data: str) -> str:\n",
    "    \"\"\"\n",
    "    Constructs the train artifact string by combining user, project, and data information. \n",
    "    The format of the return string is 'entity/project/name:version'.\n",
    "    \"\"\"\n",
    "    # entity/project/name:version\n",
    "    train_artifact=user+'/'+project+'/'+data \n",
    "    return train_artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce19aa52-7a5b-430a-9a18-9ca60ffddbca",
   "metadata": {},
   "source": [
    "## Build dataset artifact\n",
    "> Configuration file: `base`. Described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a0a20-c0e9-4495-9957-606ce4b97d57",
   "metadata": {},
   "source": [
    "### Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fcd5273-0a72-44dd-9910-8b5bb5585459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "##############################\n",
    "# 01 - DATAFRAME TO ARTIFACT #\n",
    "##############################\n",
    "def get_artifact_config_sd2a_get_auxiliar_variables(verbose : int) -> Tuple[str, str, str, AttrDict, bool, str]:\n",
    "    \"\"\"\n",
    "    Retrieves auxiliary variables necessary for the dataset artifact configuration. \n",
    "    Gathers user, project, version, and data details, along with preferences for using wandb and the wandb artifacts path.\n",
    "    Returns a tuple containing these elements.\n",
    "    \"\"\"\n",
    "    user, project, version, data = get_project_data(verbose)\n",
    "    config      = get_config(verbose)\n",
    "    data        = config.data\n",
    "    use_wandb   = config.user_preferences.use_wandb\n",
    "    wandb_path  = config.wandb.artifacts_path\n",
    "    return user, project, version, data, use_wandb, wandb_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb4590b0-3a46-4bab-ba97-9ecc4ae179e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_sd2a_check_errors(use_wandb: str, artifact_config: AttrDict, user: str, project: str):\n",
    "    \"\"\"\n",
    "    Checks for configuration errors in the dataset artifact settings. \n",
    "    Verifies the project and entity, and ensures appropriate settings are made for offline mode and missing values handling.\n",
    "    \"\"\"\n",
    "    check_project_and_entity(user, project)\n",
    "    if (\n",
    "            use_wandb   == \"offline\" \n",
    "        and artifact_config.joining_train_test  == True\n",
    "    ):\n",
    "        custom_error(\"If you're using deepvats in offline mode, set joining_train_test to False\")\n",
    "    if (\n",
    "            artifact_config.missing_values_constant is not None \n",
    "        and artifact_config.missing_values_technique is None\n",
    "    ):\n",
    "        custom_error(\"Missing values constant must be setted up only if missing_values_technique is not None. Please check base.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b570b1c-aea2-48c2-aa7b-53910bd501cd",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73afe0d8-eb7f-4431-894e-f58d8f5a7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_sd2a(verbose : int = 0) -> AttrDict:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for the dataset artifact by retrieving auxiliary variables and setting up the artifact configuration.\n",
    "    Validates the configuration to ensure it meets specific criteria and returns the final artifact configuration as an AttrDict.\n",
    "    \"\"\"\n",
    "    user, project, version, data, use_wandb, wandb_path = get_artifact_config_sd2a_get_auxiliar_variables(verbose)\n",
    "    artifact_config = AttrDict(\n",
    "        artifact_name           = data.alias,\n",
    "        csv_config              = data.csv_config,\n",
    "        data_cols               = data.cols,\n",
    "        data_fpath              = data.path,\n",
    "        date_format             = data.date_format,\n",
    "        date_offset             = data.date_offset,\n",
    "        freq                    = data.freq,\n",
    "        joining_train_test      = data.joining_train_test,\n",
    "        missing_values_technique= data.missing_values.technique,\n",
    "        missing_values_constant = data.missing_values.constant,\n",
    "        normalize_training      = data.normalize_training,\n",
    "        range_training          = data.range_training,\n",
    "        range_testing           = data.range_testing,\n",
    "        resampling_freq         = data.resampling_freq,\n",
    "        start_date              = data.start_date,\n",
    "        test_split              = data.test_split,\n",
    "        time_col                = data.time_col,\n",
    "        use_wandb               = use_wandb,\n",
    "        wandb_artifacts_path    = wandb_path\n",
    "    )\n",
    "    get_artifact_config_sd2a_check_errors(use_wandb, artifact_config, user, project)    \n",
    "    return artifact_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b90c4-ac6e-42d3-a091-f6e52e35bea8",
   "metadata": {},
   "source": [
    "## Build MVP Encoder artifacts\n",
    "> Includes also the versions for sliding window view version\n",
    "\n",
    "The configuration files `02b-encoder_mvp` contains the following information:\n",
    "### Configuration for Encoder\n",
    "> - General Configuration: `configuration`\n",
    ">   \n",
    ">   | Key        | Description                                                 | Example Value  |\n",
    ">   |------------|-------------------------------------------------------------|----------------|\n",
    ">   | `job_type` | The type of job being configured.                           | `'encoder_MVP'`|\n",
    ">   | `alias`    | Alias for the configuration, refers to the alias in base.  | Refers to *alias|\n",
    ">\n",
    "> - Wandb Configuraconfiguration.tion: `wandb`\n",
    ">   \n",
    ">   | Key        | Description                               | Example Value   |\n",
    ">   |------------|-------------------------------------------|-----------------|\n",
    ">   | `mode`     | The mode for wandb, inherited from base.  | Refers to *wdb_mode |\n",
    ">   | `group`    | Specifies if the run is part of a group.  | `null`           |\n",
    ">\n",
    "> - Specifications Coconfiguration.nfiguration: `specifications`\n",
    ">   \n",
    ">   | Key                                      | Description                              | Example Value   |\n",
    ">   |------------------------------------------|------------------------------------------|-----------------|\n",
    ">   | `batch_size`                             | The batch size for processing.           | `1024`          |\n",
    ">   | `n_epoch`                                | Number of epochs for training.           | `100`           |\n",
    ">   | `mask.future`                            | Whether to mask future samples.          | `false`         |\n",
    ">   | `mask.stateful`                          | Dictates if masking is stateful or not.  | `true`          |\n",
    ">   | `mask.sync`                              | Mask all variables at once in series.    | `false`         |\n",
    ">   | `mvp.ws1`                                | Min window size for MVP adaptable training. | `15`          |\n",
    ">   | `mvp.ws2`                                | Max window size for MVP adaptable training. | `30`          |\n",
    ">   | `mvp.r`                                  | Probability of masking in MVP.           | `0.710`         |\n",
    ">   | `mvp.valid_size`                         | Size of the validation set proportion.   | `0.2`           |\n",
    ">   | `mvp.normalize.by_sample`                | Normalization by sample indicator.       | `false`         |\n",
    ">   | `mvp.normalize.use_single_batch`         | Use single batch for normalization.      | `false`         |\n",
    ">   | `sliding_windows.stride`                 | Data points the window moves ahead.      | `15`            |\n",
    ">   | `sliding_windows.size`                   | Window size for the sliding wiow.      | `30`            |\n",
    " g window.      | `30`            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15520a-2fe0-44ae-b184-b1268dd903f2",
   "metadata": {},
   "source": [
    "### Auxiliary functions for retrieving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dcb5baa-504c-4e7c-9e57-99260a11af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "######################\n",
    "# 02b - ENCODER MVP  #\n",
    "######################\n",
    "def get_artifact_config_MVP_auxiliar_variables(verbose : int) -> Tuple[str, str, str, str, AttrDict, str, Tuple[float, float], AttrDict]:\n",
    "    \"\"\"\n",
    "    Retrieves and assembles various configuration parameters and auxiliary variables for an MVP artifact. \n",
    "    Extracts user, project, version, and data details, fetches configuration settings, and constructs training artifact strings. \n",
    "    Returns a tuple containing these elements along with MVP workspace specifications and user preferences.\n",
    "    \"\"\"\n",
    "\n",
    "    #Get neccesary variables\n",
    "    user, project, version, data = get_project_data(verbose)\n",
    "    config          = get_config(verbose, \"02b-encoder_mvp\")\n",
    "    user_preferences = config.user_preferences\n",
    "    config = config.configuration\n",
    "    train_artifact_ = get_train_artifact(user,project,data)    \n",
    "    mvp_ws1         = config.specifications.mvp.ws1\n",
    "    mvp_ws2         = config.specifications.mvp.ws2\n",
    "    mvp_ws = (mvp_ws1,mvp_ws2)\n",
    "    return user, project, version, data, config, train_artifact_, mvp_ws, user_preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77d62ca2-14da-4ab1-bbc7-3b5182dd0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_MVP_auxiliar_variables_SWV(verbose : int) -> Tuple[str, str, str, str, AttrDict, str, Tuple[float, float], AttrDict]:    \n",
    "    \"\"\"\n",
    "    Retrieves and assembles various configuration parameters and auxiliary variables for an MVP artifact. \n",
    "    Extracts user, project, version, and data details, fetches configuration settings, and constructs training artifact strings. \n",
    "    Returns a tuple containing these elements along with MVP (sliding window view) workspace specifications and user preferences.\n",
    "    \"\"\"\n",
    "    #Get neccesary variables\n",
    "    user, project, version, data = get_project_data(verbose)\n",
    "    config          = get_config(verbose, \"02c-encoder_mvp-sliding_window_view\")\n",
    "    user_preferences = config.user_preferences\n",
    "    config = config.configuration\n",
    "    train_artifact_ = get_train_artifact(user,project,data)    \n",
    "    mvp_ws1         = config.specifications.mvp.ws1\n",
    "    mvp_ws2         = config.specifications.mvp.ws2\n",
    "    mvp_ws = (mvp_ws1,mvp_ws2)\n",
    "    return user, project, version, data, config, train_artifact_, mvp_ws, user_preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff99b2-ec50-4e45-80c2-5d9c9132728f",
   "metadata": {},
   "source": [
    "### Auxiliary functions for error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa20ce-ce6b-4d0c-89c6-a1845bd695ec",
   "metadata": {},
   "source": [
    "> <span style=\"color: red;\"> TODO: Should we allow diferent project from environment ones? Should we just avoid yml configuration for this parameters and directly get dockers environment WANDB_ENTITIY and WANDB_PROJECT environment variables? </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "829c322d-a42c-4655-9e67-3b3cc2e3a9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_project_and_entity(user: str, project: str):\n",
    "    \"\"\"\n",
    "    Checks user and project are correctly setted up comparing to environment variables\n",
    "    \"\"\"\n",
    "    os_entity = os.environ['WANDB_ENTITY']\n",
    "    os_project = os.environ['WANDB_PROJECT']\n",
    "    if (os_entity != user):\n",
    "        custom_error(\"Please check .env and base.yml: entity != user os \" + os_entity + \" yaml \" + user)\n",
    "    if (os_project != project):\n",
    "        custom_error(\"Please check .env and base.yml: project differs os \" + os_project + \" yaml \" + project)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f378edb0-797d-43f1-a29c-ada58e9f7446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_MVP_check_errors(\n",
    "    artifact_config: AttrDict, \n",
    "    user: str, \n",
    "    project: str \n",
    "):\n",
    "    \"\"\"\n",
    "    Avoids incompatible wandb modes configurations.\n",
    "    Sets project = 'work-nbs' if wandb is not used for tracking.\n",
    "    \"\"\"\n",
    "    check_project_and_entity(user, project)\n",
    "        \n",
    "    if artifact_config.use_wandb:\n",
    "        if (artifact_config.analysis_mode != 'online'):\n",
    "            print_flush(\"Changing to online analysis mode - use_wandb=true\")\n",
    "            artifact_config.analysis_mode = 'online'\n",
    "    else:\n",
    "        project = 'work-nbs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ac00f-2383-40ce-885e-82503c0a5562",
   "metadata": {},
   "source": [
    "### Main configuration functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c77a575-19a9-47e7-a13a-8e81a00affe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_MVP(verbose : int = 0) -> Tuple[str, str, str, str, AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Gathers and structures the MVP artifact configuration, including user, project, version, data details, and various configuration settings.\n",
    "    Returns a tuple comprising user, project, version, data, the structured artifact configuration as an AttrDict, and the job type.\n",
    "    \"\"\"\n",
    "    user, project, version, data, config, train_artifact_, mvp_ws, user_preferences = get_artifact_config_MVP_auxiliar_variables(verbose)\n",
    "\n",
    "    artifact_config = AttrDict(\n",
    "        alias                   = config.alias,\n",
    "        analysis_mode           = config.wandb.mode, \n",
    "        batch_size              = config.specifications.batch_size,\n",
    "        epochs                  = config.specifications.n_epoch,\n",
    "        mask_future             = config.specifications.mask.future,\n",
    "        mask_stateful           = config.specifications.mask.stateful,\n",
    "        mask_sync               = config.specifications.mask.sync,\n",
    "        mvp_ws                  = mvp_ws, \n",
    "        norm_by_sample          = config.specifications.mvp.normalize.by_sample,\n",
    "        norm_use_single_batch   = config.specifications.mvp.normalize.use_single_batch,\n",
    "        r                       = config.specifications.mvp.r,\n",
    "        stride                  = config.specifications.sliding_windows.stride, \n",
    "        train_artifact          = train_artifact_, \n",
    "        valid_artifact          = None, \n",
    "        use_wandb               = user_preferences.use_wandb, \n",
    "        valid_size              = config.specifications.mvp.valid_size,\n",
    "        w                       = config.specifications.sliding_windows.size, \n",
    "        wandb_group             = config.wandb.group\n",
    "    )\n",
    "    get_artifact_config_MVP_check_errors(artifact_config, user, project)\n",
    "    return user, project, version, data, artifact_config, config.job_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7b01a75-f9fa-4069-b99b-5ea6703887c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_MVP_SWV(verbose : int = 0) -> Tuple[str, str, str, str, AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Gathers and structures the MVP_SWV artifact configuration, including user, project, version, data details, and various configuration settings.\n",
    "    Returns a tuple comprising user, project, version, data, the structured artifact configuration as an AttrDict, and the job type.\n",
    "    \"\"\"\n",
    "    user, project, version, data, config, train_artifact_, mvp_ws, user_preferences = get_artifact_config_MVP_auxiliar_variables_SWV(verbose)\n",
    "\n",
    "    artifact_config = AttrDict(\n",
    "        alias                   = config.alias,\n",
    "        analysis_mode           = config.wandb.mode, \n",
    "        batch_size              = config.specifications.batch_size,\n",
    "        epochs                  = config.specifications.n_epoch,\n",
    "        mask_future             = config.specifications.mask.future,\n",
    "        mask_stateful           = config.specifications.mask.stateful,\n",
    "        mask_sync               = config.specifications.mask.sync,\n",
    "        mvp_ws                  = mvp_ws, \n",
    "        norm_by_sample          = config.specifications.mvp.normalize.by_sample,\n",
    "        norm_use_single_batch   = config.specifications.mvp.normalize.use_single_batch,\n",
    "        r                       = config.specifications.mvp.r,\n",
    "        stride                  = config.specifications.sliding_windows.stride, \n",
    "        train_artifact          = train_artifact_, \n",
    "        valid_artifact          = None, \n",
    "        use_wandb               = user_preferences.use_wandb, \n",
    "        valid_size              = config.specifications.mvp.valid_size,\n",
    "        w                       = config.specifications.sliding_windows.size, \n",
    "        wandb_group             = config.wandb.group\n",
    "    )\n",
    "    get_artifact_config_MVP_check_errors(artifact_config, user, project)\n",
    "    return user, project, version, data, artifact_config, config.job_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7aa51f-8bbb-4445-b06a-1564e63fc32f",
   "metadata": {},
   "source": [
    "## Encoder DCAE configuration\n",
    "> The configuration file `02a-encoder_dcae` contains the following information:\n",
    "\n",
    "> - General Configurat: `configuration`ion\n",
    ">   \n",
    ">   | Key        | Description                                      | Example Value   |\n",
    ">   |------------|--------------------------------------------------|-----------------|\n",
    ">   | `job_type` | Type of the job for this configuration.           | `'encoder_DCAE'`|\n",
    ">   | `alias`    | Alias for the run, referring to the base config.  | Refers to *alias|\n",
    ">\n",
    "> - Wandb Configconfiguration.uration: `wandb`\n",
    ">   \n",
    ">   | Key       | Description                                                 | Example Value         |\n",
    ">   |-----------|-------------------------------------------------------------|-----------------------|\n",
    ">   | `use`     | Indicates whether to use wandb.                             | Refers to *use_wandb  |\n",
    ">   | `entity`  | Entity (user) for wandb configuration.                      | Refers to *wdb_user   |\n",
    ">   | `group`   | Specifies if this run should be grouped in a wandb group.   | `null`                |\n",
    ">   | `project` | Project name for wandb configuration.                       | Refers to *wdb_project|\n",
    ">\n",
    "> - Artifacconfiguration.ts Configuration: `artifacts`\n",
    ">   \n",
    ">   | Key             | Description                                                              | Example Value                                         |\n",
    ">   |-----------------|--------------------------------------------------------------------------|-------------------------------------------------------|\n",
    ">   | `train_prefix`  | Complete training artifact path in wandb.                                | Combination of *wdb_user, *wdb_project, and *alg      |\n",
    ">   | `valid.data`    | Complete name for the validation dataset artifact (null for random).     | `null`                                                |\n",
    ">   | `valid.size`    | Percentage of random items to go to validation set if `valid.data` is null. | `0.1`                                                 |\n",
    ">\n",
    "> - Specifications Configuration: `specifications`\n",
    ">   \n",
    ">   | Key            | Description                                       | Example Value   |\n",
    ">   |----------------|---------------------------------------------------|-----------------|\n",
    ">   | `batch_size`   | Batch size for processing.                        | `64`            |\n",
    ">   | `n_epoch`      | Number of epochs to train for.                    | `200`           |\n",
    ">   | `pool_szs`     | Sizes of the pooling layers in the autoencoder.   | `[2,2,4]`       |\n",
    ">   | `top_k`        | Number of elements to analyse for the top losses. | `3`             |\n",
    ">\n",
    "> - Sliding Windows Configuration: `sliding_windows`\n",
    ">   \n",
    ">   | Key       | Description                             | Example Value |\n",
    ">   |-----------|-----------------------------------------|---------------|\n",
    ">   | `stride`  | The stride for the sliding window.      | `1`           |\n",
    ">   | `size`    | Window size for the sliding window.     | `32`     configuration.     |\n",
    ">\n",
    "> - Autoencoder Configuration: `autoencoder`\n",
    ">   \n",
    ">   | Key                 | Description                                                    | Example Value  |\n",
    ">   |---------------------|----------------------------------------------------------------|----------------|\n",
    ">   | `delta`             | Size of the autoencoder bottleneck layer.                      | `60`           |\n",
    ">   | `filters.nfs`       | Number of filters in each convolutional layer of the autoencoder. | `[64,32,16]`   |\n",
    ">   | `filters.kss`       | Kernel sizes for each convolutional layer in the autoencoder.  | `[10,5,5]`     |\n",
    ">   | `filters.output_size` | The output size of the autoencoder's final layer.             | `10`           |\n",
    "inal layer.             | `10`           |\n",
    " | `10`           |\n",
    "                  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7b33aed-ba0d-4cf3-b131-c21d7b5519d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "######################\n",
    "# 02a - ENCODER DCAE #\n",
    "######################\n",
    "\n",
    "def get_artifact_config_DCAE(verbose : int = 0) -> Tuple[AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for the DCAE (Deep Convolutional AutoEncoder).\n",
    "    It fetchs the relevant settings and assembles the artifact configuration.\n",
    "    Validates the configuration to ensure correct project and entity setup and \n",
    "    returns the artifact configuration as an AttrDict along with the job type.\n",
    "    \"\"\"\n",
    "    user, project, version, data = get_project_data(verbose)\n",
    "    config = get_config(verbose, \"02a-encoder_dcae\")\n",
    "    if verbose > 0:print_flush(\"Antes de leer configuration \" + str(config))\n",
    "    config = config.configuration\n",
    "    \n",
    "    artifact_config = AttrDict(\n",
    "        alias               = config.alias,\n",
    "        use_wandb           = config.wandb.use,\n",
    "        wandb_group         = config.wandb.group,\n",
    "        wandb_entity        = config.wandb.entity,\n",
    "        wandb_project       = config.wandb.project,\n",
    "        train_artifact      = get_train_artifact(user,project,data),\n",
    "        valid_artifact      = config.artifacts.valid.data,\n",
    "        valid_size          = config.artifacts.valid.size,\n",
    "        w                   = config.specifications.sliding_windows.size,\n",
    "        stride              = config.specifications.sliding_windows.stride,\n",
    "        delta               = config.specifications.autoencoder.delta,\n",
    "        nfs                 = config.specifications.autoencoder.filters.nfs,\n",
    "        kss                 = config.specifications.autoencoder.filters.kss,\n",
    "        output_filter_size  = config.specifications.autoencoder.filters.output_size,\n",
    "        pool_szs            = config.specifications.pool_szs,\n",
    "        batch_size          = config.specifications.batch_size, \n",
    "        epochs              = config.specifications.n_epoch,\n",
    "        top_k               = config.specifications.pool_szs\n",
    "    )\n",
    "    check_project_and_entity(\n",
    "        artifact_config.wandb_entity, artifact_config.wandb_project\n",
    "    )\n",
    "    return artifact_config, config.job_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b283a86-21a6-4de5-b73f-089b4ff92628",
   "metadata": {},
   "source": [
    "## Embeddings configuration\n",
    "The specific configurations are in the file `03-embeddings`\n",
    "It has the following information:\n",
    "\n",
    "> - Job Type: `job_type`\n",
    ">   \n",
    ">   | Key       | Value         |\n",
    ">   |-----------|---------------|\n",
    ">   | `job_type`| `'embeddings'`|\n",
    ">\n",
    "> - Configuration: `configuration`\n",
    ">   - Wandb Configuration: `configuration.wandb`\n",
    ">   \n",
    ">     | Key     | Description                                          | Example Value      |\n",
    ">     |---------|------------------------------------------------------|--------------------|\n",
    ">     | `group` | Whether to group this run in a wandb group.          | Refers to *emb     |\n",
    ">     | `use`   | Indicates whether to use wandb.                      | Refers to *use_wandb |\n",
    ">     | `entity`| Entity (user) for wandb configuration.               | Refers to *wdb_user |\n",
    ">     | `project`| Project name for wandb configuration.               | Refers to *wdb_project |\n",
    ">\n",
    ">   - Encoder Configuration: `configuration.encoder`\n",
    ">   \n",
    ">     | Key                          | Description                                        | Example Value                                   |\n",
    ">     |------------------------------|----------------------------------------------------|-------------------------------------------------|\n",
    ">     | `artifacts.train.enc_prefix` | Prefix for the training artifacts.                 | Combination of *wdb_user, *wdb_project, and *alg |\n",
    ">     | `artifacts.valid`            | If none, the validation set used to train enc is used. | `null`               |\n",
    ">\n",
    ">   - Specifications Configuration: `configuration.specifications`\n",
    ">   \n",
    ">     | Key        | Description                                  | Example Value   |\n",
    ">     |------------|----------------------------------------------|-----------------|\n",
    ">     | `input_ar` | Input array, if applicable.                  | `null`          |\n",
    ">     | `cpu`      | Whether to use CPU instead of GPU.           | `false`         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db2bbe87-33e0-4cc4-9d89-ba6cf1f585c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "######################\n",
    "# 03 - EMBEDDINGS    #\n",
    "######################\n",
    "def get_artifact_config_embeddings(verbose : int = 0) -> Tuple[AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for embeddings by fetching relevant settings and building the encoder artifact configuration.\n",
    "    Validates the project and entity settings and returns the artifact configuration as an AttrDict, along with the job type.\n",
    "    \"\"\"\n",
    "\n",
    "    config = get_config(verbose, \"03a-embeddings\")\n",
    "    job_type=config.job_type\n",
    "    version = config.user_preferences.wdb.version\n",
    "    enc_artifact = build_enc_artifact(config, verbose)\n",
    "    config = config.configuration\n",
    "    artifact_config = AttrDict(\n",
    "        use_wandb       = config.wandb.use,\n",
    "        wandb_group     = config.wandb.group,\n",
    "        wandb_entity    = config.wandb.entity,\n",
    "        wandb_project   = config.wandb.project,\n",
    "        enc_artifact    = enc_artifact,\n",
    "        input_ar        = config.specifications.input_ar,\n",
    "        cpu             = config.specifications.cpu\n",
    "    )\n",
    "    check_project_and_entity(artifact_config.wandb_entity, artifact_config.wandb_project)\n",
    "    return artifact_config, job_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1d34641-83c2-4b68-b058-1145eba72c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_artifact_config_embeddings_SWV(verbose : int = 0) -> Tuple[AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for embeddings (sliding window view) by fetching relevant settings and building the encoder artifact configuration.\n",
    "    Validates the project and entity settings and returns the artifact configuration as an AttrDict, along with the job type.\n",
    "    \"\"\"\n",
    "    config = get_config(verbose, \"03b-embeddings-sliding_window_view\")\n",
    "    job_type=config.job_type\n",
    "    version = config.user_preferences.wdb.version\n",
    "    enc_artifact = build_enc_artifact(config, verbose)\n",
    "    config = config.configuration\n",
    "    artifact_config = AttrDict(\n",
    "        use_wandb       = config.wandb.use,\n",
    "        wandb_group     = config.wandb.group,\n",
    "        wandb_entity    = config.wandb.entity,\n",
    "        wandb_project   = config.wandb.project,\n",
    "        enc_artifact    = enc_artifact,\n",
    "        input_ar        = config.specifications.input_ar,\n",
    "        cpu             = config.specifications.cpu\n",
    "    )\n",
    "    check_project_and_entity(artifact_config.wandb_entity, artifact_config.wandb_project)\n",
    "    return artifact_config, job_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f5750-189c-4f6e-9652-c1747600b9bf",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction Configuration\n",
    "The specific configurations are in the file `04-dimensionality_reduction`\n",
    "It contains the following information:\n",
    "\n",
    "> - Job Type: `job_type`\n",
    ">   \n",
    ">   | Key        | Value                      |\n",
    ">   |------------|----------------------------|\n",
    ">   | `job_type` | `'dimensionality_reduction'`|\n",
    ">\n",
    "> - Configuration: `configuration`\n",
    ">   - Wandb Configuration: `configuration.wandb`\n",
    ">   \n",
    ">     | Key     | Description                                          | Example Value      |\n",
    ">     |---------|------------------------------------------------------|--------------------|\n",
    ">     | `use`   | Whether to use wandb for experiment tracking.        | Refers to *use_wandb |\n",
    ">     | `group` | Specifies whether to group this run in a wandb group. | `null`            |\n",
    ">     | `entity`| The entity to use for wandb.                         | Refers to *wdb_user |\n",
    ">     | `project`| The project to use for wandb.                       | Refers to *wdb_project |\n",
    ">\n",
    ">   - Encoder Configuration: `configuration.encoder`\n",
    ">   \n",
    ">     | Key                          | Description                                         | Example Value                                   |\n",
    ">     |------------------------------|-----------------------------------------------------|-------------------------------------------------|\n",
    ">     | `artifacts.train.enc_prefix` | Prefix for the training artifacts.                  | Combination of *wdb_user, *wdb_project, and *alg |\n",
    ">     | `artifacts.valid`            | If none, the validation set used to train enc is used. | `null`               |\n",
    ">\n",
    ">   - UMAP Configuration: `configuration.encoder.umap`\n",
    ">   \n",
    ">     | Key             | Description                 | Example Value |\n",
    ">     |-----------------|-----------------------------|---------------|\n",
    ">     | `n_neighbors`   | Number of neighbors for UMAP. | `15`          |\n",
    ">     | `min_dist`      | Minimum distance for UMAP.    | `0.1`         |\n",
    ">     | `random_state`  | Random state for UMAP.        | `1234`        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13facd5a-bfda-4973-9db2-0333c2b72a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "###################################\n",
    "# 04 - DIMENSIONALITY REDUCTION   #\n",
    "###################################\n",
    "def get_artifact_config_dimensionality_reduction(verbose : int = 0) -> Tuple[AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for dimensionality reduction tasks by fetching relevant settings, including building the encoder artifact.\n",
    "    Returns the artifact configuration as an AttrDict, along with the job type.\n",
    "    \"\"\"\n",
    "\n",
    "    config          = get_config(verbose, \"04-dimensionality_reduction\")\n",
    "    job_type        = config.job_type\n",
    "    enc_artifact = build_enc_artifact(config, verbose)\n",
    "    config = config.configuration\n",
    "    try:\n",
    "        if config.encoder.artifacts.dr is not None:\n",
    "            config.encoder.artifacts.dr = enc_artifact\n",
    "    except Exception:\n",
    "        config.encoder.artifacts.dr = None\n",
    "    \n",
    "    artifact_config = AttrDict(\n",
    "        use_wandb           = config.wandb.use, \n",
    "        wandb_group         = config.wandb.group,\n",
    "        wandb_entity        = config.wandb.entity,\n",
    "        wandb_project       = config.wandb.project,\n",
    "        valid_artifact      = config.encoder.artifacts.valid, \n",
    "        train_artifact      = enc_artifact,\n",
    "        dr_artifact         = config.encoder.artifacts.dr,\n",
    "        n_neighbors         = config.encoder.umap.n_neighbors,\n",
    "        min_dist            = config.encoder.umap.min_dist,\n",
    "        random_state        = config.encoder.umap.random_state,\n",
    "        metric              = config.encoder.umap.metric,\n",
    "        cpu_flag            = config.encoder.umap.cpu_flag\n",
    "    )\n",
    "    return artifact_config, job_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd944c-7e4a-46cd-824b-4a4c5ceae716",
   "metadata": {},
   "source": [
    "## XAI - shap reduction configuration\n",
    "> <span style=\"color: red;\"> TODO: Not yet implemented or configured... In progress.. </span>\n",
    "\n",
    "The specific configurations are in the file `05-xai_shap`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8d335f8-a63b-4dae-a2b3-db80e9ef92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "##################\n",
    "# 05 - XAI-LRP   #\n",
    "##################\n",
    "def get_artifact_config_xai_lrp(verbose : int = 0) -> Tuple[AttrDict, str]:\n",
    "    \"\"\"\n",
    "    Constructs the configuration for the XAI SHAP (Explainable Artificial Intelligence using SHAP values) analysis. \n",
    "    This includes fetching relevant settings, building the encoder artifact, and assembling the artifact configuration.\n",
    "    Returns the artifact configuration as an AttrDict, along with the job type.\n",
    "    \"\"\"\n",
    "    config          = get_config(verbose, \"05-xai_lrp\")\n",
    "    job_type        = config.job_type\n",
    "    emb_artifact    = build_emb_artifact(config, verbose)\n",
    "    config = config.configuration\n",
    "    if verbose > 0:\n",
    "        print_flush(\"-- config --\")\n",
    "        show_attrdict(config)\n",
    "        print_flush(\"-- config --\")\n",
    "    artifact_config = AttrDict(\n",
    "        use_wandb           = config.wandb.use, \n",
    "        wandb_group         = config.wandb.group,\n",
    "        wandb_entity        = config.wandb.entity,\n",
    "        wandb_project       = config.wandb.project,\n",
    "        emb_artifact        = emb_artifact,\n",
    "        job_type            = job_type,\n",
    "        allow_val_change    = config.allow_val_change,\n",
    "        stride              = config.stride\n",
    "    )\n",
    "    \n",
    "    return artifact_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186ec85-e47e-432b-992f-e8e143df9a8e",
   "metadata": {},
   "source": [
    "## Use a tested configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b5631a-e992-4bfc-80d1-038e7f73cf03",
   "metadata": {},
   "source": [
    "[Monash benchmark](https://huggingface.co/datasets/monash_tsf) datasets used configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07468ed3-5783-468b-9ebd-92ec8b9465ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Monash australian electricity demand\n",
    "monash_australian_electricity_demand_0 = AttrDict(\n",
    "    alias =\"Monash-Australian_electricity_demand\",\n",
    "    fname =\"australian_electricity_demand_dataset\",\n",
    "    #5 univariate timeseries. 0-4 can be used 1 each time\n",
    "    cols =[0],\n",
    "    ftype ='.tsf',\n",
    "    freq ='30min', \n",
    "    time_col = None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [2,336], #1h-1week | TODO: Check to ensure freq sense\n",
    "        stride = 48 #Day2Day TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec8732bc-27e0-43dd-9c27-40d6348c3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Monash sunspot:\n",
    "monash_sunspot_0 = AttrDict(\n",
    "    alias = \"sunspot\",\n",
    "    fname = \"sunspot_dataset_with_missing_values\",\n",
    "    ftype = \".tsf\",\n",
    "    cols = [],\n",
    "    freq ='1d',\n",
    "    time_col = None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [7,365], #1week-1year\n",
    "        stride = 30 #1 month TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63b24736-ba1a-4161-99fc-a328529d4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "monash_solar_4_seconds_0 = AttrDict(\n",
    "    alias =  'solar_4_seconds',\n",
    "    fname = 'solar_4_seconds_dataset',\n",
    "    ftype = '.tsf',\n",
    "    freq = '4s',\n",
    "    cols = [],\n",
    "    time_col= None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        #1 4 seconds\n",
    "        #15 1 min\n",
    "        #450 30 min - Too small for G4-0\n",
    "        #900 1h\n",
    "        ws = [450,900], #1 min - 30 min (15*60=900 = 1hora intervalos 4 secs)\n",
    "        stride = 1\n",
    "        #stride = 450\n",
    "        #stride = 10800 #1 min TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21735905-a078-4ee5-919d-f8da73f0dff0",
   "metadata": {},
   "source": [
    "[Wikipedia web traffic dataset](https://zenodo.org/records/3898474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39701d48-8d7d-4928-a528-2ecc5a71ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "wikipedia_0 = AttrDict(\n",
    "    alias=\"Wikipedia\",\n",
    "    fname=\"kaggle_web_traffic_dataset_with_missing_values\",\n",
    "    cols=[0, 1, 2, 3, 4],\n",
    "    ftype=\".tsf\",\n",
    "    freq = '1d',\n",
    "    time_col=None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [1,365], #1d-1año\n",
    "        stride = 1 #TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f195f-14d3-4f07-8180-6aaa313bee6b",
   "metadata": {},
   "source": [
    "[Traffic San Francisco](https://zenodo.org/records/3898445)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a200c2b3-daf0-441b-913b-eee9c0e8d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "traffic_san_francisco_0 = AttrDict(\n",
    "    alias=\"Traffic_SF\",\n",
    "    fname=\"traffic_hourly_dataset\",\n",
    "    ftype=\".tsf\",\n",
    "    cols=[],\n",
    "    time_col=None,\n",
    "    freq='1h',\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [1,720], #1h-1week TODO: Check\n",
    "        stride = 24 #1 day TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1db1d924-90eb-4f33-b389-d14968d883e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Monash solar 10 minutes dataset\n",
    "# Multi-dimensional\n",
    "monash_solar_10_minutes_0 = AttrDict(\n",
    "    alias =\"Monash-Solar_10_minutes\",\n",
    "    fname =\"solar_10_minutes_dataset\",\n",
    "    #5 univariate timeseries. 0-4 can be used 1 each time\n",
    "    #137 columns (each one 1 different timeserie)\n",
    "    cols =[0],\n",
    "    ftype ='.tsf',\n",
    "    freq ='10min',\n",
    "    time_col = None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        #1 month 4320 -> Too big for G4 (GPU-0)\n",
    "        #1 day 144 -> Ok\n",
    "        #1 week 1008 -> Ok\n",
    "        #15 days 2160 -> Ok\n",
    "        ws = [1,2160],\n",
    "        stride = 144 #1d by 1d\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5484725-82ac-4d55-b58d-ded17db5c577",
   "metadata": {},
   "source": [
    "### Other public datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f223b4-6523-4ff3-bea9-ca765b012547",
   "metadata": {},
   "source": [
    "#### Electricity Transformer Temperature\n",
    "[ETDataset](https://paperswithcode.com/dataset/ett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb020bd-6e7d-4668-a24b-b529f93e2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "etth1_0 = AttrDict(\n",
    "    alias=\"ETTh1\",\n",
    "    fname=\"ETTh1\",\n",
    "    ftype=\".csv\",\n",
    "    cols=[],\n",
    "    time_col=0,\n",
    "    freq='1h',\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [1,720], #1 h - 1 month TODO:Check\n",
    "        stride = 24 #1 day TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5630ad06-9eaf-4198-a9dc-f0e71f3a020d",
   "metadata": {},
   "source": [
    "#### Stumpy\n",
    "##### [Semantic segmentation TitlABP](https://stumpy.readthedocs.io/en/latest/Tutorial_Semantic_Segmentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3b0e373-a1b7-4fb1-96ca-1992815305e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "stumpy_abp_0 = AttrDict(\n",
    "    alias=\"TitlABP\",\n",
    "    fname=\"Semantic_Segmentation_TiltABP\",\n",
    "    ftype=\".csv\",\n",
    "    cols=[],\n",
    "    freq=\"1s\",\n",
    "    time_col=0, \n",
    "    mvp = AttrDict(\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        ws = [60,3600], #1min-1h TODO:Check\n",
    "        stride = 60 #1 min TODO: Check\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 512,\n",
    "        n_epoch = 100,\n",
    "        stride = 48,\n",
    "        w      = 224,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebbe03-d4af-44dd-bd38-d56d1e8dec90",
   "metadata": {},
   "source": [
    "##### [Multi-dimensional toy data](https://zenodo.org/records/4294932)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ec755c5-2013-4802-bb54-24b2642d01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "stumpy_toy_0 = AttrDict(\n",
    "    alias=\"toy\",\n",
    "    fname=\"toy\",\n",
    "    ftype=\".csv\",\n",
    "    cols=[],\n",
    "    freq=\"1s\",\n",
    "    time_col=None,\n",
    "    mvp = AttrDict(\n",
    "        batch_size = 32,\n",
    "        n_epoch = 100,\n",
    "        ws = [10,30], \n",
    "        stride = 1\n",
    "    ),\n",
    "    dcae = AttrDict(#TODO: Check\n",
    "        batch_size = 64,\n",
    "        n_epoch = 200,\n",
    "        stride = 1,\n",
    "        w      = 30,\n",
    "        delta  = 60,\n",
    "        nfs    = [64, 32, 16],\n",
    "        kss     = [10, 5, 5],\n",
    "        output_filter_size = 10,\n",
    "        top_k = [2,2,4],\n",
    "        pool_szs = [2,2,4]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f4fa2-dfbf-499d-a2c1-b588dc64940c",
   "metadata": {},
   "source": [
    "### Get tested configuration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dee0968-1f1c-4a67-90a0-fa998ab854b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "tested_configs = {\n",
    "    'monash_australian_electricity_demand_0': monash_australian_electricity_demand_0,\n",
    "    'monash_solar_4_seconds_0': monash_solar_4_seconds_0,\n",
    "    'wikipedia_0': wikipedia_0,\n",
    "    'traffic_san_francisco_0': traffic_san_francisco_0,\n",
    "    'monash_solar_10_minutes_0': monash_solar_10_minutes_0,\n",
    "    'etth1_0': etth1_0,\n",
    "    'stumpy_abp_0':  stumpy_abp_0,\n",
    "    'stumpy_toy_0': stumpy_toy_0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ff14373-c56f-4a5a-a787-12191196b091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_attrdict(\n",
    "    dict            : AttrDict, \n",
    "    # Print options\n",
    "    print_to_path   : bool          = False,\n",
    "    print_path      : str           = \"~/data/logs/logs.txt\",\n",
    "    print_mode      : str           = 'a'\n",
    "):\n",
    "    for key, value in dict.items():\n",
    "        print_flush(f\"{key}: {value}\", print_to_path = print_to_path, print_path = print_path, print_mode = print_mode, print_time = print_to_path)\n",
    "        print_mode = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "daadf0fe-5d0b-4068-86dd-1f17cbf30d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_available_configs():\n",
    "    print_flush(\"Available datasets: \")\n",
    "    i = 0\n",
    "    for key, val in tested_configs.items():\n",
    "        print_flush(f\"{i} - {key}\")\n",
    "        i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb13402c-46e1-4960-9836-daf47839d3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets: \n",
      "0 - monash_australian_electricity_demand_0\n",
      "1 - monash_solar_4_seconds_0\n",
      "2 - wikipedia_0\n",
      "3 - traffic_san_francisco_0\n",
      "4 - monash_solar_10_minutes_0\n",
      "5 - etth1_0\n",
      "6 - stumpy_abp_0\n",
      "7 - stumpy_toy_0\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "show_available_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "887c90c2-1704-40e2-bd94-1d811e3c1a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'traffic_san_francisco_0'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide \n",
    "list(tested_configs.items())[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "668cc6c8-bb89-4b2e-a598-0ee0f3f788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def show_config(id: int = 0):\n",
    "    show_attrdict(list(tested_configs.items())[id][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "154878de-f491-44e6-86ea-d8bfa8fa4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alias: toy\n",
      "fname: toy\n",
      "ftype: .csv\n",
      "cols: []\n",
      "freq: 1s\n",
      "time_col: None\n",
      "mvp: {'batch_size': 32, 'n_epoch': 100, 'ws': [10, 30], 'stride': 1}\n",
      "dcae: {'batch_size': 64, 'n_epoch': 200, 'stride': 1, 'w': 30, 'delta': 60, 'nfs': [64, 32, 16], 'kss': [10, 5, 5], 'output_filter_size': 10, 'top_k': [2, 2, 4], 'pool_szs': [2, 2, 4]}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "show_config(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e12be39-14e7-41bf-8459-6ed47c708792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_tested_config(\n",
    "    id: int = 0,\n",
    "    verbose = 0,\n",
    "):\n",
    "    if verbose > 0:show_config(id)\n",
    "    return list(tested_configs.items())[id][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3cfac15b-b9e7-4781-8397-5b363c800a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alias: toy\n",
      "fname: toy\n",
      "ftype: .csv\n",
      "cols: []\n",
      "freq: 1s\n",
      "time_col: None\n",
      "mvp: {'batch_size': 32, 'n_epoch': 100, 'ws': [10, 30], 'stride': 1}\n",
      "dcae: {'batch_size': 64, 'n_epoch': 200, 'stride': 1, 'w': 30, 'delta': 60, 'nfs': [64, 32, 16], 'kss': [10, 5, 5], 'output_filter_size': 10, 'top_k': [2, 2, 4], 'pool_szs': [2, 2, 4]}\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "show_attrdict(get_tested_config(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585097af-8862-45f4-a259-986a3a10d963",
   "metadata": {},
   "source": [
    "### Force tested configuration functions\n",
    "#### 01 - Dataset Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0d400a03-8f9b-4308-90cc-735dc92c1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_colored(\n",
    "    key, \n",
    "    modified_val, \n",
    "    modified, \n",
    "    both:bool=False, \n",
    "    original_val=0,\n",
    "    missing_in_modified=False,\n",
    "    missing_in_original=False\n",
    "):\n",
    "    if missing_in_modified:\n",
    "        color = \"\\033[91m\\033[1m\"  # Red and bold\n",
    "    elif missing_in_original:\n",
    "        color = \"\\033[93m\\033[1m\"  # Orange and bold\n",
    "    else:\n",
    "        color = \"\\033[94m\" if modified else \"\"\n",
    "    reset = \"\\033[0m\"\n",
    "    \n",
    "    if modified and both:\n",
    "        print_flush(f\"{color}{key}: {original_val}{reset} -> {modified_val}{reset}\")\n",
    "    elif missing_in_modified:\n",
    "        print_flush(f\"{color}{key} is missing in modified dict | {original_val} {reset}\")\n",
    "    elif missing_in_original:\n",
    "        print_flush(f\"{color}{key} is missing in original dict | {modified_val} {reset}\")\n",
    "    else:\n",
    "        print_flush(f\"{color}{key}: {modified_val}{reset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eeb494df-c5ee-4cde-a635-c6aea6521fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb2e52a0-0492-4529-b050-ef5384534f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_resampling_frequency(\n",
    "    freq: str,\n",
    "    frequency_factor:int = 1,\n",
    "    verbose = 0\n",
    "):\n",
    "    if verbose > 0:\n",
    "        print_flush(\"--> Frequency factor resampling frequency\")\n",
    "        print_flush(\"Freq factor: \", frequency_factor)\n",
    "    freq_new = pd.to_timedelta(freq)\n",
    "    freq_new = freq_new*frequency_factor\n",
    "    if verbose > 0:\n",
    "        print_flush(\"freq_original: \", freq)\n",
    "        print_flush(\"freq_new: \", freq_new)\n",
    "    suffix = \"-\"\n",
    "    resampling_freq=\"\"\n",
    "    if freq_new.days > 0 and freq_new.seconds == pd.to_timedelta(0,'s'):\n",
    "        suffix  = str(freq_new.days)+'d'\n",
    "        resampling_freq = str(freq_new.days)+'D'\n",
    "    elif freq_new.seconds % 3600 == 0:\n",
    "        hours = (freq_new.seconds // 3600) + freq_new.days*24\n",
    "        suffix = str(hours)+'h'\n",
    "        resampling_freq = str(hours) + 'H'\n",
    "    elif freq_new.seconds % 60 == 0: \n",
    "        minutes = (freq_new.seconds // 60) + (freq_new.days*24*60)\n",
    "        suffix = str(minutes)+'m'\n",
    "        resampling_freq = str(minutes)+'T'\n",
    "    else: \n",
    "        seconds = freq_new.seconds + freq_new.days*24*60*60 \n",
    "        suffix = str(seconds)+'s'\n",
    "        resampling_freq = str(seconds)+'S'\n",
    "    if verbose > 0:\n",
    "        print_flush(\"suffix: \", suffix)\n",
    "        print_flush(\"resampling_freq: \", resampling_freq)\n",
    "        print_flush(\"Frequency factor resampling frequency -->\")\n",
    "    return (suffix, resampling_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e365d0-504d-40bf-a9bd-0afe04e5f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Frequency factor resampling frequency\n",
      "Freq factor:  15\n",
      "freq_original:  4s\n",
      "freq_new:  0 days 00:01:00\n",
      "suffix:  1m\n",
      "resampling_freq:  1T\n",
      "Frequency factor resampling frequency -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1m', '1T')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_resampling_frequency(freq = \"4s\",frequency_factor = 15,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98c21a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Frequency factor resampling frequency\n",
      "Freq factor:  15\n",
      "freq_original:  4s\n",
      "freq_new:  0 days 00:01:00\n",
      "suffix:  1m\n",
      "resampling_freq:  1T\n",
      "Frequency factor resampling frequency -->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1m', '1T')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_resampling_frequency(freq = \"4s\",frequency_factor = 15,verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84e8fb00-673e-4955-a0c0-9933b2ae3750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def frequency_factor_config(\n",
    "    config: AttrDict, \n",
    "    frequency_factor:int = 1,\n",
    "    frequency_factor_change_alias: bool = True,\n",
    "    verbose = 0\n",
    "):\n",
    "    if verbose > 0:\n",
    "        print_flush(\"--> Frequency factor config\")\n",
    "        print_flush(\"Freq factor: \", frequency_factor)\n",
    "        print_flush(\"frequency_factor_change_alias: \", frequency_factor_change_alias)\n",
    "    suffix, config.resampling_freq = get_resampling_frequency(config.freq, frequency_factor, verbose)\n",
    "\n",
    "    if frequency_factor_change_alias:\n",
    "        #filename = config.data_fpath.split(\".tsf\", 1)[0]\n",
    "        config.artifact_name = config.artifact_name+\"-\"+suffix\n",
    "        #config.data_fpath = filename+\"-\"+suffix+\".tsf\"\n",
    "\n",
    "    if verbose > 0:    \n",
    "        print_flush(\"resampling_freq: \", config.freq)\n",
    "        print_flush(\"name: \", config.artifact_name)\n",
    "        print_flush(\"path: \", config.data_fpath)    \n",
    "        print_flush(\"Frequency factor config -->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4baad8bd-bd77-42c9-a6c7-d6bfc9d9cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def diff_attrdict(\n",
    "    dict_original: AttrDict, \n",
    "    dict_modified: AttrDict,\n",
    "    both: bool = False\n",
    "):\n",
    "    all_keys = set(dict_original.keys()) | set(dict_modified.keys())\n",
    "    for key in all_keys:\n",
    "        in_original = key in dict_original\n",
    "        in_modified = key in dict_modified\n",
    "        \n",
    "        if in_original and in_modified:\n",
    "            modified = dict_original[key] != dict_modified[key]\n",
    "            print_colored(\n",
    "                key, \n",
    "                modified_val = dict_modified[key], \n",
    "                modified = modified, \n",
    "                both = both, \n",
    "                original_val=dict_original[key]\n",
    "            )\n",
    "        elif in_original:\n",
    "            # Key is missing in dict_modified\n",
    "            print_colored(key, modified_val=None, modified=True, missing_in_modified=True)\n",
    "        else:\n",
    "            # Key is missing in dict_original\n",
    "            print_colored(key, modified_val=dict_modified[key], modified=True, missing_in_original=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afda84a7-9ea0-4166-8cb7-c36376d4619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from copy import deepcopy\n",
    "def force_artifact_config_sd2a(\n",
    "    config: AttrDict,\n",
    "    id:int = 0, \n",
    "    verbose = 0,\n",
    "    both = False,\n",
    "    frequency_factor = 1, \n",
    "    frequency_factor_change_alias = True\n",
    "):\n",
    "    to_set = get_tested_config(id)\n",
    "    if verbose > 0:\n",
    "        config_before = deepcopy(config)\n",
    "        print_flush(\"Selecting \", list(tested_configs.items())[id][0])\n",
    "    config.artifact_name = to_set.alias\n",
    "    config.data_cols = to_set.cols\n",
    "    config.data_fpath= \"~/data/\"+to_set.fname+to_set.ftype\n",
    "    config.freq=to_set.freq\n",
    "    config.time_col = to_set.time_col\n",
    "    config.csv_config = {}\n",
    "    joining_train_test= False,\n",
    "    missing_values_constant= None,\n",
    "    missing_values_technique= None,\n",
    "    normalize_training= False,\n",
    "    range_testing= None,\n",
    "    range_training= None,\n",
    "    resampling_freq= None,\n",
    "    start_date= None,\n",
    "    test_split= None,\n",
    "    if frequency_factor > 1: \n",
    "        frequency_factor_config(config, frequency_factor, frequency_factor_change_alias, verbose)\n",
    "    if verbose > 0:\n",
    "        diff_attrdict(\n",
    "            dict_original=config_before, \n",
    "            dict_modified=config, \n",
    "            both = both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec3910fc-3d6d-4b94-8d3d-5c18edb3029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sd2a_config = AttrDict(\n",
    "    artifact_name= \"Monash-Australian_electricity\",\n",
    "    csv_config= {},\n",
    "    data_cols= [0],\n",
    "    data_fpath= '~/data/australian_electricity_demand_dataset.csv',\n",
    "    date_format= '%Y-%m-%d %H:%M:%S',\n",
    "    date_offset= None,\n",
    "    freq= '1s',\n",
    "    joining_train_test= False,\n",
    "    missing_values_constant= None,\n",
    "    missing_values_technique= None,\n",
    "    normalize_training= False,\n",
    "    range_testing= None,\n",
    "    range_training= None,\n",
    "    resampling_freq= None,\n",
    "    start_date= None,\n",
    "    test_split= None,\n",
    "    time_col= None,\n",
    "    use_wandb= True,\n",
    "    wandb_artifacts_path='./data/wandb_artifacts'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d1c4265-1294-406f-978a-d3e9bd2a4547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting  stumpy_toy_0\n",
      "--> Frequency factor config\n",
      "Freq factor:  2\n",
      "frequency_factor_change_alias:  True\n",
      "--> Frequency factor resampling frequency\n",
      "Freq factor:  2\n",
      "freq_original:  1s\n",
      "freq_new:  0 days 00:00:02\n",
      "suffix:  2s\n",
      "resampling_freq:  2S\n",
      "Frequency factor resampling frequency -->\n",
      "resampling_freq:  1s\n",
      "name:  toy-2s\n",
      "path:  ~/data/toy.csv\n",
      "Frequency factor config -->\n",
      "start_date: None\u001b[0m\n",
      "normalize_training: False\u001b[0m\n",
      "test_split: None\u001b[0m\n",
      "date_format: %Y-%m-%d %H:%M:%S\u001b[0m\n",
      "range_testing: None\u001b[0m\n",
      "freq: 1s\u001b[0m\n",
      "\u001b[94martifact_name: Monash-Australian_electricity\u001b[0m -> toy-2s\u001b[0m\n",
      "\u001b[94mresampling_freq: None\u001b[0m -> 2S\u001b[0m\n",
      "wandb_artifacts_path: ./data/wandb_artifacts\u001b[0m\n",
      "range_training: None\u001b[0m\n",
      "date_offset: None\u001b[0m\n",
      "csv_config: {}\u001b[0m\n",
      "\u001b[94mdata_fpath: ~/data/australian_electricity_demand_dataset.csv\u001b[0m -> ~/data/toy.csv\u001b[0m\n",
      "joining_train_test: False\u001b[0m\n",
      "time_col: None\u001b[0m\n",
      "\u001b[94mdata_cols: [0]\u001b[0m -> []\u001b[0m\n",
      "missing_values_technique: None\u001b[0m\n",
      "missing_values_constant: None\u001b[0m\n",
      "use_wandb: True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "force_artifact_config_sd2a(\n",
    "    config = sd2a_config, \n",
    "    id = 7, \n",
    "    verbose=1, \n",
    "    both=True, \n",
    "    frequency_factor = 2, \n",
    "    frequency_factor_change_alias = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5964ed-1cf6-469b-bced-2cbe262b20bd",
   "metadata": {},
   "source": [
    "#### 02(bc) Encoder MVP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d483893-35a9-4be2-8324-9e099af4bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_artifact_string(s:string) -> tuple[string, string, string]:\n",
    "    # Divide la cadena en dos partes usando ':'\n",
    "    path, version = s.split(':')\n",
    "\n",
    "    # Divide la parte del path en sus componentes\n",
    "    parts = path.rsplit('/', 1)\n",
    "\n",
    "    # Retorna los componentes separados\n",
    "    return parts[0] + '/', parts[1], version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52395a2d-0dbd-4226-82b0-7853ff555572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mi-santamaria/deepvats/', 'Monash-Australian_electricity_demand', 'latest')\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "result = split_artifact_string(\"mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\")\n",
    "print_flush(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb828063-26b2-43fb-ab92-e60a0ee0db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def force_artifact_config_mvp(\n",
    "    config: AttrDict,\n",
    "    id:int = 0, \n",
    "    verbose = 0,\n",
    "    both = False,\n",
    "    frequency_factor = 1,\n",
    "    frequency_factor_change_alias = False,\n",
    "):\n",
    "    to_set = get_tested_config(id)\n",
    "    if verbose > 0:\n",
    "        config_before = deepcopy(config)\n",
    "        \n",
    "    force_artifact_config_sd2a(\n",
    "        config = config, \n",
    "        id = id, \n",
    "        verbose = 0, \n",
    "        both = False, \n",
    "        frequency_factor = frequency_factor, \n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )\n",
    "\n",
    "    config.alias = to_set.alias\n",
    "    config.batch_size = to_set.mvp.batch_size\n",
    "    config.epochs = to_set.mvp.n_epoch\n",
    "    \n",
    "    config.mask_future = False \n",
    "    config.mask_stateful = True\n",
    "    config.mask_sync = False\n",
    "    config.norm_by_sample = False\n",
    "    config.norm_use_by_single_batch = False,\n",
    "    config.r = 0.71\n",
    "    \n",
    "    config.stride=to_set.mvp.stride\n",
    "    path,_,version = split_artifact_string(config.train_artifact)\n",
    "    config.train_artifact=path+config.artifact_name+\":\"+version\n",
    "\n",
    "    config.valid_size = 0.2\n",
    "    \n",
    "    config.mvp_ws= to_set.mvp.ws\n",
    "    config.w = config.mvp_ws[1]\n",
    "    \n",
    "    if verbose > 0:\n",
    "        diff_attrdict(\n",
    "            dict_original=config_before, \n",
    "            dict_modified=config, \n",
    "            both = both\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f50ac55c-f49e-4b4f-ad56-1acce64907ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "mvp_config = AttrDict(\n",
    "    alias = \"Monash-Australian_electric\",\n",
    "    artifact_name=\"Monash-Australian_electricity\",\n",
    "    analysis_mode=\"online\",\n",
    "    batch_size=2,\n",
    "    epochs=1,\n",
    "    mask_future=False,\n",
    "    mask_stateful=True,\n",
    "    mask_sync=False,\n",
    "    mvp_ws=(150, 33),\n",
    "    norm_by_sample=False,\n",
    "    norm_use_single_batch=False,\n",
    "    r=0.71,\n",
    "    stride=13,\n",
    "    train_artifact=\"mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\",\n",
    "    valid_artifact=None,\n",
    "    use_wandb=True,\n",
    "    valid_size=0.2,\n",
    "    w=30,\n",
    "    wandb_group=None,\n",
    "    data_cols= [],\n",
    "    data_fpath= \"~/data/kaggle_web_traffic_dataset_with_missing_values.tsf\",\n",
    "    freq= '1d',\n",
    "    time_col= 'None'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c71f660-eae7-4d0a-bdb7-7733c3ba8fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mtrain_artifact: mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\u001b[0m -> mi-santamaria/deepvats/solar_4_seconds-20s:latest\u001b[0m\n",
      "mask_stateful: True\u001b[0m\n",
      "\u001b[93m\u001b[1mnorm_use_by_single_batch is missing in original dict | (False,) \u001b[0m\n",
      "valid_artifact: None\u001b[0m\n",
      "\u001b[94mepochs: 1\u001b[0m -> 100\u001b[0m\n",
      "\u001b[94mfreq: 1d\u001b[0m -> 4s\u001b[0m\n",
      "wandb_group: None\u001b[0m\n",
      "\u001b[94martifact_name: Monash-Australian_electricity\u001b[0m -> solar_4_seconds-20s\u001b[0m\n",
      "\u001b[93m\u001b[1mresampling_freq is missing in original dict | 20S \u001b[0m\n",
      "norm_by_sample: False\u001b[0m\n",
      "norm_use_single_batch: False\u001b[0m\n",
      "\u001b[94malias: Monash-Australian_electric\u001b[0m -> solar_4_seconds\u001b[0m\n",
      "valid_size: 0.2\u001b[0m\n",
      "\u001b[93m\u001b[1mcsv_config is missing in original dict | {} \u001b[0m\n",
      "\u001b[94mbatch_size: 2\u001b[0m -> 512\u001b[0m\n",
      "r: 0.71\u001b[0m\n",
      "\u001b[94mdata_fpath: ~/data/kaggle_web_traffic_dataset_with_missing_values.tsf\u001b[0m -> ~/data/solar_4_seconds_dataset.tsf\u001b[0m\n",
      "analysis_mode: online\u001b[0m\n",
      "mask_sync: False\u001b[0m\n",
      "\u001b[94mmvp_ws: (150, 33)\u001b[0m -> [450, 900]\u001b[0m\n",
      "\u001b[94mtime_col: None\u001b[0m -> None\u001b[0m\n",
      "\u001b[94mstride: 13\u001b[0m -> 450\u001b[0m\n",
      "data_cols: []\u001b[0m\n",
      "mask_future: False\u001b[0m\n",
      "\u001b[94mw: 30\u001b[0m -> 900\u001b[0m\n",
      "use_wandb: True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "force_artifact_config_mvp(mvp_config, 1, True, True, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11c0fb05-4e42-4750-a554-a1f923076c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def force_artifact_config_dcae(\n",
    "    config: AttrDict,\n",
    "    id:int = 0, \n",
    "    verbose = 0,\n",
    "    both = False,\n",
    "    frequency_factor = 1,\n",
    "    frequency_factor_change_alias = False,\n",
    "):\n",
    "    to_set = get_tested_config(id)\n",
    "    if verbose > 0:\n",
    "        config_before = deepcopy(config)\n",
    "        \n",
    "    force_artifact_config_sd2a(\n",
    "        config = config, \n",
    "        id = id, \n",
    "        verbose = 0, \n",
    "        both = False, \n",
    "        frequency_factor = frequency_factor, \n",
    "        frequency_factor_change_alias = frequency_factor_change_alias\n",
    "    )\n",
    "\n",
    "    config.alias = to_set.alias\n",
    "   \n",
    "    config.batch_size = to_set.dcae.batch_size\n",
    "    config.epochs = to_set.dcae.n_epoch    \n",
    "    config.r = 0.71\n",
    "    config.stride=to_set.dcae.stride\n",
    "    path,_,version = split_artifact_string(config.train_artifact)\n",
    "    config.train_artifact=path+config.artifact_name+\":\"+version\n",
    "\n",
    "    config.valid_size = 0.2\n",
    "    config.w = to_set.dcae.w\n",
    "    config.delta = to_set.dcae.delta\n",
    "    config.nfs = to_set.dcae.nfs\n",
    "    config.kss = to_set.dcae.kss\n",
    "    config.output_filter_size = to_set.dcae.kss\n",
    "    config.top_k = to_set.dcae.top_k\n",
    "    config.pool_szs = to_set.dcae.pool_szs\n",
    "    \n",
    "    if verbose > 0:\n",
    "        diff_attrdict(\n",
    "            dict_original=config_before, \n",
    "            dict_modified=config, \n",
    "            both = both\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8e45d01-2db6-4f6a-9b70-9b97929f92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "dcae_config = AttrDict(\n",
    "    alias = \"Monash-Australian_electric\",\n",
    "    artifact_name=\"Monash-Australian_electricity\",\n",
    "    analysis_mode=\"online\",\n",
    "    batch_size=2,\n",
    "    epochs=1,\n",
    "    r=0,\n",
    "    stride=1,\n",
    "    train_artifact=\"mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\",\n",
    "    valid_artifact=None,\n",
    "    use_wandb=True,\n",
    "    valid_size=0.2,\n",
    "    w=30,\n",
    "    nfs = [],\n",
    "    kss = [], \n",
    "    output_filter_size = 1,\n",
    "    top_k = [],\n",
    "    delta = 6,\n",
    "    wandb_group=None,\n",
    "    data_cols= [],\n",
    "    data_fpath= \"~/data/kaggle_web_traffic_dataset_with_missing_values.tsf\",\n",
    "    freq= '1d',\n",
    "    time_col= 'None'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34ad57e1-cc30-460d-966e-81d10e7ab2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mtrain_artifact: mi-santamaria/deepvats/Monash-Australian_electricity_demand:latest\u001b[0m -> mi-santamaria/deepvats/solar_4_seconds-20s:latest\u001b[0m\n",
      "valid_artifact: None\u001b[0m\n",
      "\u001b[94mepochs: 1\u001b[0m -> 100\u001b[0m\n",
      "\u001b[94mkss: []\u001b[0m -> [10, 5, 5]\u001b[0m\n",
      "\u001b[94mfreq: 1d\u001b[0m -> 4s\u001b[0m\n",
      "wandb_group: None\u001b[0m\n",
      "\u001b[94martifact_name: Monash-Australian_electricity\u001b[0m -> solar_4_seconds-20s\u001b[0m\n",
      "\u001b[93m\u001b[1mresampling_freq is missing in original dict | 20S \u001b[0m\n",
      "\u001b[94mtop_k: []\u001b[0m -> [2, 2, 4]\u001b[0m\n",
      "\u001b[93m\u001b[1mpool_szs is missing in original dict | [2, 2, 4] \u001b[0m\n",
      "\u001b[94malias: Monash-Australian_electric\u001b[0m -> solar_4_seconds\u001b[0m\n",
      "valid_size: 0.2\u001b[0m\n",
      "\u001b[93m\u001b[1mcsv_config is missing in original dict | {} \u001b[0m\n",
      "\u001b[94mnfs: []\u001b[0m -> [64, 32, 16]\u001b[0m\n",
      "\u001b[94mbatch_size: 2\u001b[0m -> 512\u001b[0m\n",
      "\u001b[94mr: 0\u001b[0m -> 0.71\u001b[0m\n",
      "\u001b[94mdata_fpath: ~/data/kaggle_web_traffic_dataset_with_missing_values.tsf\u001b[0m -> ~/data/solar_4_seconds_dataset.tsf\u001b[0m\n",
      "analysis_mode: online\u001b[0m\n",
      "\u001b[94moutput_filter_size: 1\u001b[0m -> [10, 5, 5]\u001b[0m\n",
      "\u001b[94mtime_col: None\u001b[0m -> None\u001b[0m\n",
      "\u001b[94mstride: 1\u001b[0m -> 48\u001b[0m\n",
      "data_cols: []\u001b[0m\n",
      "\u001b[94mdelta: 6\u001b[0m -> 60\u001b[0m\n",
      "\u001b[94mw: 30\u001b[0m -> 224\u001b[0m\n",
      "use_wandb: True\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| hide \n",
    "force_artifact_config_dcae(dcae_config, 1, True, True, 5, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
